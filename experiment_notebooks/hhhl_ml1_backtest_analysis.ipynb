{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import quantstats as qs\n",
    "import pickle\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/df_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2006-06-19 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2009-07-17 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2012-08-08 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2015-09-15 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2018-08-29 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2021-07-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>End</td>\n",
       "      <td>2006-06-16 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>2024-10-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Period</td>\n",
       "      <td>1618 days 00:00:00</td>\n",
       "      <td>775 days 00:00:00</td>\n",
       "      <td>2393 days 00:00:00</td>\n",
       "      <td>772 days 00:00:00</td>\n",
       "      <td>3165 days 00:00:00</td>\n",
       "      <td>779 days 00:00:00</td>\n",
       "      <td>3944 days 00:00:00</td>\n",
       "      <td>745 days 00:00:00</td>\n",
       "      <td>4689 days 00:00:00</td>\n",
       "      <td>721 days 00:00:00</td>\n",
       "      <td>5410 days 00:00:00</td>\n",
       "      <td>816 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Start Value</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>End Value</td>\n",
       "      <td>290.77508190136484</td>\n",
       "      <td>242.1327157507496</td>\n",
       "      <td>3209.085046269844</td>\n",
       "      <td>306.1137161032776</td>\n",
       "      <td>3462.3753343410394</td>\n",
       "      <td>240.99195732631208</td>\n",
       "      <td>6015.343803012408</td>\n",
       "      <td>211.483926360233</td>\n",
       "      <td>8837.467898317822</td>\n",
       "      <td>230.3408712190465</td>\n",
       "      <td>88960.23579451621</td>\n",
       "      <td>546.3912384357037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                    0                    1                    2  \\\n",
       "0        Start  2000-01-10 00:00:00  2006-06-19 00:00:00  2000-01-10 00:00:00   \n",
       "1          End  2006-06-16 00:00:00  2009-07-16 00:00:00  2009-07-16 00:00:00   \n",
       "2       Period   1618 days 00:00:00    775 days 00:00:00   2393 days 00:00:00   \n",
       "3  Start Value                100.0                100.0                100.0   \n",
       "4    End Value   290.77508190136484    242.1327157507496    3209.085046269844   \n",
       "\n",
       "                     3                    4                    5  \\\n",
       "0  2009-07-17 00:00:00  2000-01-10 00:00:00  2012-08-08 00:00:00   \n",
       "1  2012-08-07 00:00:00  2012-08-07 00:00:00  2015-09-14 00:00:00   \n",
       "2    772 days 00:00:00   3165 days 00:00:00    779 days 00:00:00   \n",
       "3                100.0                100.0                100.0   \n",
       "4    306.1137161032776   3462.3753343410394   240.99195732631208   \n",
       "\n",
       "                     6                    7                    8  \\\n",
       "0  2000-01-10 00:00:00  2015-09-15 00:00:00  2000-01-10 00:00:00   \n",
       "1  2015-09-14 00:00:00  2018-08-28 00:00:00  2018-08-28 00:00:00   \n",
       "2   3944 days 00:00:00    745 days 00:00:00   4689 days 00:00:00   \n",
       "3                100.0                100.0                100.0   \n",
       "4    6015.343803012408     211.483926360233    8837.467898317822   \n",
       "\n",
       "                     9                   10                   11  \n",
       "0  2018-08-29 00:00:00  2000-01-10 00:00:00  2021-07-15 00:00:00  \n",
       "1  2021-07-12 00:00:00  2021-07-12 00:00:00  2024-10-10 00:00:00  \n",
       "2    721 days 00:00:00   5410 days 00:00:00    816 days 00:00:00  \n",
       "3                100.0                100.0                100.0  \n",
       "4    230.3408712190465    88960.23579451621    546.3912384357037  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# existing stats from pf.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>train_end_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_expectancy</th>\n",
       "      <th>test_expectancy</th>\n",
       "      <th>train_sharpe_ratio</th>\n",
       "      <th>test_sharpe_ratio</th>\n",
       "      <th>train_calmar_ratio</th>\n",
       "      <th>test_calmar_ratio</th>\n",
       "      <th>train_omega_ratio</th>\n",
       "      <th>test_omega_ratio</th>\n",
       "      <th>train_sortino_ratio</th>\n",
       "      <th>test_sortino_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2006-06-19 00:00:00</td>\n",
       "      <td>2006-06-16 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>290.775082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.295368</td>\n",
       "      <td>3.384112</td>\n",
       "      <td>0.975684</td>\n",
       "      <td>0.980142</td>\n",
       "      <td>0.426440</td>\n",
       "      <td>1.747438</td>\n",
       "      <td>1.250014</td>\n",
       "      <td>1.329824</td>\n",
       "      <td>1.565967</td>\n",
       "      <td>1.576195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2009-07-17 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2393.0</td>\n",
       "      <td>772.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3209.085046</td>\n",
       "      <td>...</td>\n",
       "      <td>22.932115</td>\n",
       "      <td>4.528574</td>\n",
       "      <td>1.106845</td>\n",
       "      <td>1.087510</td>\n",
       "      <td>1.464769</td>\n",
       "      <td>2.977417</td>\n",
       "      <td>1.300569</td>\n",
       "      <td>1.300668</td>\n",
       "      <td>1.773852</td>\n",
       "      <td>1.705708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2012-08-08 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>3165.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3462.375334</td>\n",
       "      <td>...</td>\n",
       "      <td>17.675559</td>\n",
       "      <td>2.660226</td>\n",
       "      <td>1.027141</td>\n",
       "      <td>0.934371</td>\n",
       "      <td>0.643946</td>\n",
       "      <td>2.646347</td>\n",
       "      <td>1.278662</td>\n",
       "      <td>1.269234</td>\n",
       "      <td>1.646453</td>\n",
       "      <td>1.471992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2015-09-15 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>3944.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6015.343803</td>\n",
       "      <td>...</td>\n",
       "      <td>20.183980</td>\n",
       "      <td>1.548388</td>\n",
       "      <td>0.992068</td>\n",
       "      <td>0.865236</td>\n",
       "      <td>0.711761</td>\n",
       "      <td>2.601962</td>\n",
       "      <td>1.268213</td>\n",
       "      <td>1.203659</td>\n",
       "      <td>1.569741</td>\n",
       "      <td>1.292896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2018-08-29 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>4689.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8837.467898</td>\n",
       "      <td>...</td>\n",
       "      <td>28.689557</td>\n",
       "      <td>1.780570</td>\n",
       "      <td>0.939279</td>\n",
       "      <td>0.958932</td>\n",
       "      <td>0.607345</td>\n",
       "      <td>3.048036</td>\n",
       "      <td>1.259467</td>\n",
       "      <td>1.221953</td>\n",
       "      <td>1.485741</td>\n",
       "      <td>1.477511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2021-07-15 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>2024-10-10 00:00:00</td>\n",
       "      <td>5410.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88960.235795</td>\n",
       "      <td>...</td>\n",
       "      <td>223.869176</td>\n",
       "      <td>7.262950</td>\n",
       "      <td>0.987662</td>\n",
       "      <td>2.365588</td>\n",
       "      <td>1.082136</td>\n",
       "      <td>7.418604</td>\n",
       "      <td>1.281594</td>\n",
       "      <td>1.646318</td>\n",
       "      <td>1.568137</td>\n",
       "      <td>4.174921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_test_combination          train_start           test_start  \\\n",
       "0                     0.0  2000-01-10 00:00:00  2006-06-19 00:00:00   \n",
       "1                     1.0  2000-01-10 00:00:00  2009-07-17 00:00:00   \n",
       "2                     2.0  2000-01-10 00:00:00  2012-08-08 00:00:00   \n",
       "3                     3.0  2000-01-10 00:00:00  2015-09-15 00:00:00   \n",
       "4                     4.0  2000-01-10 00:00:00  2018-08-29 00:00:00   \n",
       "5                     5.0  2000-01-10 00:00:00  2021-07-15 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-06-16 00:00:00  2009-07-16 00:00:00        1618.0        775.0   \n",
       "1  2009-07-16 00:00:00  2012-08-07 00:00:00        2393.0        772.0   \n",
       "2  2012-08-07 00:00:00  2015-09-14 00:00:00        3165.0        779.0   \n",
       "3  2015-09-14 00:00:00  2018-08-28 00:00:00        3944.0        745.0   \n",
       "4  2018-08-28 00:00:00  2021-07-12 00:00:00        4689.0        721.0   \n",
       "5  2021-07-12 00:00:00  2024-10-10 00:00:00        5410.0        816.0   \n",
       "\n",
       "   train_start_value  test_start_value  train_end_value  ...  \\\n",
       "0              100.0             100.0       290.775082  ...   \n",
       "1              100.0             100.0      3209.085046  ...   \n",
       "2              100.0             100.0      3462.375334  ...   \n",
       "3              100.0             100.0      6015.343803  ...   \n",
       "4              100.0             100.0      8837.467898  ...   \n",
       "5              100.0             100.0     88960.235795  ...   \n",
       "\n",
       "   train_expectancy  test_expectancy  train_sharpe_ratio  test_sharpe_ratio  \\\n",
       "0          1.295368         3.384112            0.975684           0.980142   \n",
       "1         22.932115         4.528574            1.106845           1.087510   \n",
       "2         17.675559         2.660226            1.027141           0.934371   \n",
       "3         20.183980         1.548388            0.992068           0.865236   \n",
       "4         28.689557         1.780570            0.939279           0.958932   \n",
       "5        223.869176         7.262950            0.987662           2.365588   \n",
       "\n",
       "   train_calmar_ratio  test_calmar_ratio  train_omega_ratio  test_omega_ratio  \\\n",
       "0            0.426440           1.747438           1.250014          1.329824   \n",
       "1            1.464769           2.977417           1.300569          1.300668   \n",
       "2            0.643946           2.646347           1.278662          1.269234   \n",
       "3            0.711761           2.601962           1.268213          1.203659   \n",
       "4            0.607345           3.048036           1.259467          1.221953   \n",
       "5            1.082136           7.418604           1.281594          1.646318   \n",
       "\n",
       "   train_sortino_ratio  test_sortino_ratio  \n",
       "0             1.565967            1.576195  \n",
       "1             1.773852            1.705708  \n",
       "2             1.646453            1.471992  \n",
       "3             1.569741            1.292896  \n",
       "4             1.485741            1.477511  \n",
       "5             1.568137            4.174921  \n",
       "\n",
       "[6 rows x 57 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_existing_stats(df_stats):\n",
    "    df_stats = df_stats.set_index('Unnamed: 0')\n",
    "    df_stats.index.name = None\n",
    "    \n",
    "    transformed_rows = []\n",
    "\n",
    "    # Iterate over pairs of columns (train-test combinations)\n",
    "    for i in range(0, df_stats.shape[1], 2):\n",
    "        train_col = i\n",
    "        test_col = i + 1\n",
    "        \n",
    "        # Create a dictionary to store values for the current combination\n",
    "        row = {}\n",
    "        \n",
    "        # Set the combination number\n",
    "        row['train_test_combination'] = i // 2\n",
    "        \n",
    "        # Add train and test statistics to the row with new column names\n",
    "        for stat in df_stats.index:\n",
    "            row[f'train_{stat.lower().replace(\" \", \"_\")}'] = df_stats.loc[stat, str(train_col)]\n",
    "            row[f'test_{stat.lower().replace(\" \", \"_\")}'] = df_stats.loc[stat, str(test_col)]\n",
    "        \n",
    "        # Append the row to the transformed rows list\n",
    "        transformed_rows.append(row)\n",
    "\n",
    "    # Create a new DataFrame from the transformed rows\n",
    "    df_transformed = pd.DataFrame(transformed_rows)\n",
    "    excluded_columns = {'train_start', 'test_start', 'train_end', 'test_end', 'train_period', 'test_period'}\n",
    "    for col in df_transformed.columns:\n",
    "        if 'duration' not in col and col not in excluded_columns:\n",
    "            df_transformed[col] = df_transformed[col].astype(float)\n",
    "        elif (col in excluded_columns and 'period' in col) or ('duration' in col):\n",
    "            # Convert period columns to float days\n",
    "            df_transformed[col] = pd.to_timedelta(df_transformed[col]).dt.total_seconds() / (24 * 3600)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "process_existing_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trend strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Creating a sample time series data (you can replace this with your stock data)\n",
    "dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')\n",
    "np.random.seed(42)\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "\n",
    "# Create a DataFrame with the dates and prices\n",
    "df_test = pd.DataFrame({'Date': dates, 'Price': price_data})\n",
    "df_test.set_index('Date', inplace=True)\n",
    "\n",
    "# Function to calculate the trend slope over the last 'window_size' observations\n",
    "def calculate_trend_slope(train_start, train_end, test_start, test_end, window_size):\n",
    "    series = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/df_price.csv', index = 'Date')['Adj Close']\n",
    "    if len(series) < window_size:\n",
    "        raise ValueError(f\"Series length is less than the window size {window_size}\")\n",
    "    \n",
    "    # Extract the last 'window_size' observations\n",
    "    y = series[-window_size:]\n",
    "    x = np.arange(len(y))  # Time indices\n",
    "\n",
    "    # Perform linear regression to calculate slope\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "    return slope\n",
    "\n",
    "# # Calculate the slope for different window sizes\n",
    "# window_50 = calculate_trend_slope(df_test['Price'], window_size=50)\n",
    "# window_200 = calculate_trend_slope(df_test['Price'], window_size=200)\n",
    "\n",
    "# print(f\"Trend slope for last 50 observations: {window_50}\")\n",
    "# print(f\"Trend slope for last 200 observations: {window_200}\")\n",
    "\n",
    "# # Plot the data and trend line\n",
    "# plt.plot(df_test.index, df_test['Price'], label='Price')\n",
    "\n",
    "# # Overlay the trend line for the last 50 observations\n",
    "# x_50 = np.arange(50)\n",
    "# y_trend_50 = window_50 * x_50 + df_test['Price'][-50].mean()  # Adjust to match the series' mean\n",
    "# plt.plot(df_test.index[-50:], y_trend_50, color='red', label='Trend (last 50)')\n",
    "\n",
    "# # Overlay the trend line for the last 200 observations\n",
    "# x_200 = np.arange(200)\n",
    "# y_trend_200 = window_200 * x_200 + df_test['Price'][-200].mean()  # Adjust to match the series' mean\n",
    "# plt.plot(df_test.index[-200:], y_trend_200, color='green', label='Trend (last 200)')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seasonal strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal strength for the last 200 days: 0.0016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def calculate_seasonal_strength(series, window_size, period):\n",
    "    \n",
    "    # Get the last 'window_size' observations from the series\n",
    "    series_window = series.tail(window_size)\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the seasonal and residual components\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Calculate variance of seasonal and residual components, ignoring NaNs\n",
    "    seasonal_variance = np.nanvar(seasonal)\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    \n",
    "    # Normalize by the mean or standard deviation of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (seasonal_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate normalized seasonal strength: seasonal_variance / residual_variance, normalized by the mean of the series\n",
    "    normalized_seasonal_strength = seasonal_variance / (seasonal_variance + residual_variance) / mean_of_series # maybe remove (/s var + res var)\n",
    "    \n",
    "    return normalized_seasonal_strength\n",
    "\n",
    "\n",
    "\n",
    "# Example: Generating sample time series data (replace this with actual stock data)\n",
    "dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')\n",
    "np.random.seed(42)\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "\n",
    "# Create a DataFrame with the dates and prices\n",
    "df_test = pd.DataFrame({'Date': dates, 'Price': price_data})\n",
    "df_test.set_index('Date', inplace=True)\n",
    "\n",
    "# Use the function to calculate seasonal strength for the last 200 days\n",
    "window_size = 200\n",
    "period = 30  # Monthly seasonality, 7 weekly, 365 yearly\n",
    "\n",
    "seasonal_strength_value = calculate_seasonal_strength(df_test['Price'], window_size, period)\n",
    "\n",
    "print(f\"Seasonal strength for the last {window_size} days: {seasonal_strength_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cyclical strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclical Strength: 0.019642711876708036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def calculate_cyclical_strength(series, window_size, period, moving_avg_window=30):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the cyclical component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the cyclical strength.\n",
    "    period (int): The period of the cycle (e.g., 365 for annual cycles in daily data).\n",
    "    moving_avg_window (int): Window size for the moving average to approximate the cyclical component.\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized cyclical strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the last 'window_size' observations from the series\n",
    "    series_window = pd.Series(series).tail(window_size)\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the residual component (contains cyclical + noise components)\n",
    "    residual = result.resid\n",
    "\n",
    "    # Apply moving average to the residuals to approximate the cyclical component\n",
    "    cyclical = residual.rolling(window=moving_avg_window, center=True).mean()\n",
    "\n",
    "    # Calculate variance of the cyclical component and the residuals, ignoring NaNs\n",
    "    cyclical_variance = np.nanvar(cyclical)\n",
    "    residual_variance = np.nanvar(residual - cyclical)  # Noise component variance\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (cyclical_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate normalized cyclical strength as a proportion of total variability\n",
    "    normalized_cyclical_strength = (cyclical_variance / (cyclical_variance + residual_variance)) / mean_of_series\n",
    "    \n",
    "    return normalized_cyclical_strength\n",
    "\n",
    "\n",
    "# Example usage with synthetic data (assuming 'series' and 'period' are predefined)\n",
    "period = 365  # Assuming yearly seasonality for daily data\n",
    "window = 30  # The window size for the moving average to remove short-term noise\n",
    "window_size = 1000\n",
    "cyclical_strength_value = calculate_cyclical_strength(price_data, window_size, period, window)\n",
    "\n",
    "print(f\"Cyclical Strength: {cyclical_strength_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noise strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise strength for the last 200 days: 0.0484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def calculate_noise_strength(series, window_size, period):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the noise (residual) component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the noise strength.\n",
    "    period (int): The period of the seasonality (e.g., 365 for yearly seasonality on daily data).\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized noise strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the last 'window_size' observations from the series\n",
    "    series_window = series.tail(window_size)\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the seasonal and residual components\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Calculate variance of seasonal and residual components, ignoring NaNs\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if mean_of_series == 0 or residual_variance == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate normalized noise strength\n",
    "    normalized_noise_strength = residual_variance / mean_of_series\n",
    "    \n",
    "    return normalized_noise_strength\n",
    "\n",
    "# Example usage: \n",
    "dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')\n",
    "np.random.seed(42)\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "\n",
    "df_test = pd.DataFrame({'Date': dates, 'Price': price_data})\n",
    "df_test.set_index('Date', inplace=True)\n",
    "\n",
    "window_size = 200\n",
    "period = 30  # Set an appropriate period based on data frequency\n",
    "\n",
    "noise_strength_value = calculate_noise_strength(df_test['Price'], window_size, period)\n",
    "print(f\"Noise strength for the last {window_size} days: {noise_strength_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fractal efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Efficiency Ratio: 0.0084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_overall_efficiency_ratio(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the overall Efficiency Ratio (Fractal Efficiency) of a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    \n",
    "    Returns:\n",
    "    float: Efficiency Ratio for the entire series.\n",
    "    \"\"\"\n",
    "    series_window = pd.Series(series).tail(window)\n",
    "    # Calculate the Net Change over the entire series\n",
    "    net_change = abs(series_window.iloc[-1] - series_window.iloc[0])\n",
    "    \n",
    "    # Calculate the Total Change across all points in the series\n",
    "    total_change = series_window.diff().abs().sum()\n",
    "    \n",
    "    # Calculate the Efficiency Ratio\n",
    "    if total_change == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    \n",
    "    efficiency_ratio = net_change / total_change\n",
    "    return efficiency_ratio\n",
    "\n",
    "# Example usage\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "# Calculate the Efficiency Ratio for the entire series\n",
    "overall_efficiency_ratio = calculate_overall_efficiency_ratio(series, 50)\n",
    "print(f\"Overall Efficiency Ratio: {overall_efficiency_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# current price as % of max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last price percent of overall max price: 0.9063\n"
     ]
    }
   ],
   "source": [
    "def calculate_percent_of_max(series):\n",
    "    s_max = series.max()\n",
    "    perc_of_max = series.tail(1)[0]/s_max\n",
    "    \n",
    "    return perc_of_max\n",
    "\n",
    "# Example usage\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "perc_of_max = calculate_percent_of_max(series)\n",
    "print(f\"Last price percent of overall max price: {perc_of_max:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max drawdown & max drawdown duration of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price max drawdown: -0.1392\n",
      "Price max drawdown duration as % of whole period: 0.61\n"
     ]
    }
   ],
   "source": [
    "def calculate_max_drawdown(series):\n",
    "    s = series.pct_change()\n",
    "    max_drawdown = qs.stats.max_drawdown(s)\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]/len(series)\n",
    "    \n",
    "    return max_drawdown, max_drawdown_duration\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "max_d, max_d_dur = calculate_max_drawdown(series)\n",
    "print(f\"Price max drawdown: {max_d:.4f}\")\n",
    "print(f\"Price max drawdown duration as % of whole period: {max_d_dur}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standard deviation of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price standard deviation: 0.0843\n"
     ]
    }
   ],
   "source": [
    "def calculate_std(series):\n",
    "    std = series.std()/series.mean()\n",
    "    return std\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "std = calculate_std(series)\n",
    "print(f\"Price standard deviation: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc stats of number of days between trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.00043687199650502403, 0.027959807776321538, 0.004621645857763674, 0.006057585335997416)\n"
     ]
    }
   ],
   "source": [
    "def calculate_period_without_trades(df_trades):\n",
    "    df_trades['exit_ts_shift'] = df_trades['Exit Timestamp'].shift(1)\n",
    "    period_length = (df_trades['Exit Timestamp'].max() - df_trades['Entry Timestamp'].min()).days\n",
    "    df_trades['days_between_trade'] = ((df_trades['Entry Timestamp'] - df_trades['exit_ts_shift']).dt.days)/period_length\n",
    "    min_days = df_trades['days_between_trade'].min()\n",
    "    max_days = df_trades['days_between_trade'].max()\n",
    "    mean_days = df_trades['days_between_trade'].mean()\n",
    "    std_days = df_trades['days_between_trade'].std()    \n",
    "    \n",
    "    return min_days, max_days, mean_days, std_days\n",
    "\n",
    "df_trades = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/train_0_trades.csv')\n",
    "df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "desc_days_trades = calculate_period_without_trades(df_trades)\n",
    "print(desc_days_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc of trades per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 1.763157894736842, 1.220583697008734)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_trades_per_period(df_trades):\n",
    "    df_trades.set_index('Entry Timestamp', inplace = True)\n",
    "    df_g = df_trades.resample('M')['PnL'].count()\n",
    "    min_trades = df_g.min()\n",
    "    max_trades = df_g.max()\n",
    "    mean_trades = df_g.mean()\n",
    "    std_trades = df_g.std()\n",
    "    return min_trades, max_trades, mean_trades, std_trades\n",
    "\n",
    "df_trades = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/train_0_trades.csv')\n",
    "df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "calculate_trades_per_period(df_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc of trades returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2761519761776835,\n",
       " 0.2499999999999999,\n",
       " 0.011366545607989977,\n",
       " 0.07980345577870791)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_trades_returns(df_trades):\n",
    "    min_trades = df_trades['Return'].min()\n",
    "    max_trades = df_trades['Return'].max()\n",
    "    mean_trades = df_trades['Return'].mean()\n",
    "    std_trades = df_trades['Return'].std()\n",
    "    \n",
    "    return min_trades, max_trades, mean_trades, std_trades\n",
    "\n",
    "df_trades = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/train_0_trades.csv')\n",
    "df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "calculate_trades_returns(df_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perc of current drawdown period from max drawdown period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perc_drawdown_period(series):\n",
    "    s = series.pct_change()\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]\n",
    "    current_drawdown_duration = (cummax == cummax.tail(1)[0]).sum()\n",
    "    return current_drawdown_duration/max_drawdown_duration\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "perc_drawdown_duration = calculate_perc_drawdown_period(series)\n",
    "perc_drawdown_duration\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation of cumulative returns with price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4697479615044177"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_price_returns_corr(series, df_trades):\n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    df = pd.concat([series, (returns + 1).cumprod()], axis = 1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace = True)\n",
    "    corr = df.corr().iloc[0,1]\n",
    "    return corr, df\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "df_trades = pd.DataFrame({'Return': np.random.rand(len(dates)), 'Exit Timestamp': dates})\n",
    "\n",
    "corr, df_out = calculate_price_returns_corr(series, df_trades)\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc stats of rolling correlation of returns and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7456334463291172,\n",
       " 0.9526217962634782,\n",
       " 0.3283485567919696,\n",
       " 0.5163894125931645)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_price_returns_rolling_corr(series, df_trades, window):\n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    df = pd.concat([series, (returns + 1).cumprod()], axis = 1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace = True)\n",
    "    price = df.iloc[:, 0]\n",
    "    trades_returns = df.iloc[:, 1]\n",
    "    rolling_corr = price.rolling(window = window).corr(trades_returns).dropna()\n",
    "    min_corr = rolling_corr.min()\n",
    "    max_corr = rolling_corr.max()\n",
    "    mean_corr = rolling_corr.mean()\n",
    "    std_corr = rolling_corr.std()\n",
    "    return min_corr, max_corr, mean_corr, std_corr\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "df_trades = pd.DataFrame({'Return': np.random.rand(len(dates)), 'Exit Timestamp': dates})\n",
    "\n",
    "t = calculate_price_returns_rolling_corr(series, df_trades, 10)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outlier counts in trades returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_outlier_cnts(df_trades, threshold):\n",
    "    outliers_cnt = (df_trades['Return'] <= threshold).sum()\n",
    "    return outliers_cnt/len(df_trades)\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "df_trades = pd.DataFrame({'Return': np.random.rand(len(dates)), 'Exit Timestamp': dates})\n",
    "\n",
    "calculate_outlier_cnts(df_trades, -0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model train stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'False_precision': 0.99,\n",
       " 'False_recall': 0.97,\n",
       " 'False_f1_score': 0.98,\n",
       " 'True_precision': 0.94,\n",
       " 'True_recall': 0.98,\n",
       " 'True_f1_score': 0.96,\n",
       " 'accuracy': 0.97,\n",
       " 'macro avg_precision': 0.96,\n",
       " 'macro avg_recall': 0.97,\n",
       " 'macro avg_f1_score': 0.97,\n",
       " 'weighted avg_precision': 0.97,\n",
       " 'weighted avg_recall': 0.97,\n",
       " 'weighted avg_f1_score': 0.97}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_class_report(report):\n",
    "    pattern = r'(\\b(?:False|True|accuracy|macro avg|weighted avg)\\b)\\s+([\\d.]*)\\s+([\\d.]+)\\s+([\\d.]+)'\n",
    "    matches = re.findall(pattern, report)\n",
    "\n",
    "    # Create dictionary to store metrics\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for match in matches:\n",
    "        label, precision, recall, f1_score = match\n",
    "        if label == 'accuracy':  # Only one value for accuracy\n",
    "            metrics_dict[label] = float(recall)\n",
    "        else:\n",
    "            if precision:  # Ignore empty precision values for rows without it\n",
    "                metrics_dict[f\"{label}_precision\"] = float(precision)\n",
    "            metrics_dict[f\"{label}_recall\"] = float(recall)\n",
    "            metrics_dict[f\"{label}_f1_score\"] = float(f1_score)\n",
    "    return metrics_dict\n",
    "train_stuff = pickle.load(open('../outputs_hhhl_ml1/before_good26_rest/ORCL/train_stuff.pickle', 'rb'))\n",
    "report = train_stuff['train_class_reports'][0]\n",
    "get_class_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'before_good26_rest'\n",
    "experiment_path = f\"../outputs_hhhl_ml1/{experiment_name}\"\n",
    "\n",
    "symbols = [name for name in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_symbols_dfs = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    symbol_path = f\"{experiment_path}/{symbol}\"\n",
    "    df_stats = pd.read_csv(f\"{symbol_path}/backtest/df_stats.csv\")\n",
    "    df_symbol = process_existing_stats(df_stats)\n",
    "    df_symbol['symbol'] = symbol\n",
    "    colnames = [df_symbol.columns[-1]] + list(df_symbol.columns[:-1])\n",
    "    df_symbol = df_symbol[colnames]\n",
    "    list_symbols_dfs.append(df_symbol)\n",
    "df = pd.concat(list_symbols_dfs, axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1052, 58)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_expectancy</th>\n",
       "      <th>test_expectancy</th>\n",
       "      <th>train_sharpe_ratio</th>\n",
       "      <th>test_sharpe_ratio</th>\n",
       "      <th>train_calmar_ratio</th>\n",
       "      <th>test_calmar_ratio</th>\n",
       "      <th>train_omega_ratio</th>\n",
       "      <th>test_omega_ratio</th>\n",
       "      <th>train_sortino_ratio</th>\n",
       "      <th>test_sortino_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.654207</td>\n",
       "      <td>2.855433</td>\n",
       "      <td>1.274314</td>\n",
       "      <td>0.657814</td>\n",
       "      <td>1.879271</td>\n",
       "      <td>0.933827</td>\n",
       "      <td>1.343739</td>\n",
       "      <td>1.337054</td>\n",
       "      <td>2.098648</td>\n",
       "      <td>1.044396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.243531</td>\n",
       "      <td>2.866706</td>\n",
       "      <td>1.220968</td>\n",
       "      <td>1.141061</td>\n",
       "      <td>2.072756</td>\n",
       "      <td>3.707172</td>\n",
       "      <td>1.339096</td>\n",
       "      <td>1.280292</td>\n",
       "      <td>1.997439</td>\n",
       "      <td>1.792124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.706777</td>\n",
       "      <td>0.869280</td>\n",
       "      <td>1.178688</td>\n",
       "      <td>0.747883</td>\n",
       "      <td>2.326982</td>\n",
       "      <td>1.059748</td>\n",
       "      <td>1.334222</td>\n",
       "      <td>1.176476</td>\n",
       "      <td>1.927009</td>\n",
       "      <td>1.124046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>137.488440</td>\n",
       "      <td>1.802699</td>\n",
       "      <td>1.130173</td>\n",
       "      <td>0.751539</td>\n",
       "      <td>2.269650</td>\n",
       "      <td>2.176509</td>\n",
       "      <td>1.326110</td>\n",
       "      <td>1.187576</td>\n",
       "      <td>1.839687</td>\n",
       "      <td>1.130974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>169.810563</td>\n",
       "      <td>1.601596</td>\n",
       "      <td>1.048429</td>\n",
       "      <td>0.921876</td>\n",
       "      <td>1.858248</td>\n",
       "      <td>2.395654</td>\n",
       "      <td>1.312682</td>\n",
       "      <td>1.179903</td>\n",
       "      <td>1.698057</td>\n",
       "      <td>1.374492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TJX</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2021-12-28 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>2024-10-17 00:00:00</td>\n",
       "      <td>5528.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>409.687788</td>\n",
       "      <td>1.725069</td>\n",
       "      <td>1.025770</td>\n",
       "      <td>1.419325</td>\n",
       "      <td>1.954004</td>\n",
       "      <td>3.024418</td>\n",
       "      <td>1.310984</td>\n",
       "      <td>1.335233</td>\n",
       "      <td>1.667681</td>\n",
       "      <td>2.168212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-02-06 00:00:00</td>\n",
       "      <td>2018-08-02 00:00:00</td>\n",
       "      <td>2018-08-01 00:00:00</td>\n",
       "      <td>2021-05-28 00:00:00</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.517477</td>\n",
       "      <td>2.053002</td>\n",
       "      <td>1.052592</td>\n",
       "      <td>1.298221</td>\n",
       "      <td>2.405298</td>\n",
       "      <td>4.799122</td>\n",
       "      <td>1.238280</td>\n",
       "      <td>1.334131</td>\n",
       "      <td>1.570554</td>\n",
       "      <td>2.099449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-02-06 00:00:00</td>\n",
       "      <td>2021-06-01 00:00:00</td>\n",
       "      <td>2021-05-28 00:00:00</td>\n",
       "      <td>2024-10-17 00:00:00</td>\n",
       "      <td>2093.0</td>\n",
       "      <td>852.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.895038</td>\n",
       "      <td>2.595950</td>\n",
       "      <td>1.172641</td>\n",
       "      <td>1.889017</td>\n",
       "      <td>2.529929</td>\n",
       "      <td>4.206680</td>\n",
       "      <td>1.262204</td>\n",
       "      <td>1.427613</td>\n",
       "      <td>1.755537</td>\n",
       "      <td>3.193254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CSX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2006-08-03 00:00:00</td>\n",
       "      <td>2009-11-09 00:00:00</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>823.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.738321</td>\n",
       "      <td>6.089717</td>\n",
       "      <td>1.499536</td>\n",
       "      <td>1.592117</td>\n",
       "      <td>2.247327</td>\n",
       "      <td>2.529441</td>\n",
       "      <td>1.475021</td>\n",
       "      <td>1.383238</td>\n",
       "      <td>2.634410</td>\n",
       "      <td>2.648810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CSX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2009-11-10 00:00:00</td>\n",
       "      <td>2009-11-09 00:00:00</td>\n",
       "      <td>2012-11-27 00:00:00</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.785562</td>\n",
       "      <td>4.555247</td>\n",
       "      <td>1.411191</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>1.532272</td>\n",
       "      <td>2.078037</td>\n",
       "      <td>1.444509</td>\n",
       "      <td>1.362127</td>\n",
       "      <td>2.447735</td>\n",
       "      <td>1.617255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "5    TJX                     5.0  2000-01-07 00:00:00  2021-12-28 00:00:00   \n",
       "0    ZTS                     0.0  2013-02-06 00:00:00  2018-08-02 00:00:00   \n",
       "1    ZTS                     1.0  2013-02-06 00:00:00  2021-06-01 00:00:00   \n",
       "0    CSX                     0.0  2000-01-07 00:00:00  2006-08-04 00:00:00   \n",
       "1    CSX                     1.0  2000-01-07 00:00:00  2009-11-10 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "5  2021-12-27 00:00:00  2024-10-17 00:00:00        5528.0        706.0   \n",
       "0  2018-08-01 00:00:00  2021-05-28 00:00:00        1382.0        711.0   \n",
       "1  2021-05-28 00:00:00  2024-10-17 00:00:00        2093.0        852.0   \n",
       "0  2006-08-03 00:00:00  2009-11-09 00:00:00        1652.0        823.0   \n",
       "1  2009-11-09 00:00:00  2012-11-27 00:00:00        2475.0        767.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_expectancy  \\\n",
       "0              100.0             100.0  ...          6.654207   \n",
       "1              100.0             100.0  ...         16.243531   \n",
       "2              100.0             100.0  ...         56.706777   \n",
       "3              100.0             100.0  ...        137.488440   \n",
       "4              100.0             100.0  ...        169.810563   \n",
       "5              100.0             100.0  ...        409.687788   \n",
       "0              100.0             100.0  ...          3.517477   \n",
       "1              100.0             100.0  ...          6.895038   \n",
       "0              100.0             100.0  ...         13.738321   \n",
       "1              100.0             100.0  ...         45.785562   \n",
       "\n",
       "   test_expectancy  train_sharpe_ratio  test_sharpe_ratio  train_calmar_ratio  \\\n",
       "0         2.855433            1.274314           0.657814            1.879271   \n",
       "1         2.866706            1.220968           1.141061            2.072756   \n",
       "2         0.869280            1.178688           0.747883            2.326982   \n",
       "3         1.802699            1.130173           0.751539            2.269650   \n",
       "4         1.601596            1.048429           0.921876            1.858248   \n",
       "5         1.725069            1.025770           1.419325            1.954004   \n",
       "0         2.053002            1.052592           1.298221            2.405298   \n",
       "1         2.595950            1.172641           1.889017            2.529929   \n",
       "0         6.089717            1.499536           1.592117            2.247327   \n",
       "1         4.555247            1.411191           0.994682            1.532272   \n",
       "\n",
       "   test_calmar_ratio  train_omega_ratio  test_omega_ratio  \\\n",
       "0           0.933827           1.343739          1.337054   \n",
       "1           3.707172           1.339096          1.280292   \n",
       "2           1.059748           1.334222          1.176476   \n",
       "3           2.176509           1.326110          1.187576   \n",
       "4           2.395654           1.312682          1.179903   \n",
       "5           3.024418           1.310984          1.335233   \n",
       "0           4.799122           1.238280          1.334131   \n",
       "1           4.206680           1.262204          1.427613   \n",
       "0           2.529441           1.475021          1.383238   \n",
       "1           2.078037           1.444509          1.362127   \n",
       "\n",
       "   train_sortino_ratio  test_sortino_ratio  \n",
       "0             2.098648            1.044396  \n",
       "1             1.997439            1.792124  \n",
       "2             1.927009            1.124046  \n",
       "3             1.839687            1.130974  \n",
       "4             1.698057            1.374492  \n",
       "5             1.667681            2.168212  \n",
       "0             1.570554            2.099449  \n",
       "1             1.755537            3.193254  \n",
       "0             2.634410            2.648810  \n",
       "1             2.447735            1.617255  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend_slope(train_start, train_end, symbol, experiment_name, window_size):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if len(series) < window_size:\n",
    "        raise ValueError(f\"Series length is less than the window size {window_size}\")\n",
    "    \n",
    "    # Extract the last 'window_size' observations\n",
    "    y = series[-window_size:]\n",
    "    x = np.arange(len(y))  # Time indices\n",
    "\n",
    "    # Perform linear regression to calculate slope\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "    return slope\n",
    "df['train_slope_250'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 250), axis = 'columns')\n",
    "df['train_slope_100'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 100), axis = 'columns')\n",
    "df['train_slope_50'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 50), axis = 'columns')\n",
    "df['train_slope_20'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 20), axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seasonal_strength(train_start, train_end, symbol, experiment_name, window_size, period):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "    seasonal_variance = np.nanvar(seasonal)\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (seasonal_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    normalized_seasonal_strength = seasonal_variance / (seasonal_variance + residual_variance) / mean_of_series # maybe remove (/s var + res var)\n",
    "    \n",
    "    return normalized_seasonal_strength\n",
    "\n",
    "df['train_seasonal_strength_all_30'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_seasonal_strength_all_7'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')\n",
    "df['train_seasonal_strength_1y_30'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_seasonal_strength_1y_7'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_sortino_ratio</th>\n",
       "      <th>test_sortino_ratio</th>\n",
       "      <th>train_slope_250</th>\n",
       "      <th>train_slope_100</th>\n",
       "      <th>train_slope_50</th>\n",
       "      <th>train_slope_20</th>\n",
       "      <th>train_seasonal_strength_all_30</th>\n",
       "      <th>train_seasonal_strength_all_7</th>\n",
       "      <th>train_seasonal_strength_1y_30</th>\n",
       "      <th>train_seasonal_strength_1y_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.098648</td>\n",
       "      <td>1.044396</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.997439</td>\n",
       "      <td>1.792124</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.005656</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.034292</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.927009</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>-0.011919</td>\n",
       "      <td>-0.024633</td>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.003238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.839687</td>\n",
       "      <td>1.130974</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>-0.010845</td>\n",
       "      <td>-0.002406</td>\n",
       "      <td>-0.092302</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.698057</td>\n",
       "      <td>1.374492</td>\n",
       "      <td>0.041044</td>\n",
       "      <td>-0.096869</td>\n",
       "      <td>0.086110</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_sortino_ratio  \\\n",
       "0              100.0             100.0  ...             2.098648   \n",
       "1              100.0             100.0  ...             1.997439   \n",
       "2              100.0             100.0  ...             1.927009   \n",
       "3              100.0             100.0  ...             1.839687   \n",
       "4              100.0             100.0  ...             1.698057   \n",
       "\n",
       "   test_sortino_ratio  train_slope_250  train_slope_100  train_slope_50  \\\n",
       "0            1.044396         0.002756        -0.002667        0.007090   \n",
       "1            1.792124         0.015681         0.000486       -0.005656   \n",
       "2            1.124046         0.024224        -0.011919       -0.024633   \n",
       "3            1.130974         0.009027        -0.010845       -0.002406   \n",
       "4            1.374492         0.041044        -0.096869        0.086110   \n",
       "\n",
       "   train_slope_20  train_seasonal_strength_all_30  \\\n",
       "0        0.015246                        0.002305   \n",
       "1        0.007873                        0.001074   \n",
       "2        0.070955                        0.001343   \n",
       "3       -0.092302                        0.000374   \n",
       "4        0.061747                        0.000220   \n",
       "\n",
       "   train_seasonal_strength_all_7  train_seasonal_strength_1y_30  \\\n",
       "0                       0.001010                       0.020462   \n",
       "1                       0.000352                       0.034292   \n",
       "2                       0.000313                       0.011229   \n",
       "3                       0.000404                       0.005329   \n",
       "4                       0.000175                       0.000893   \n",
       "\n",
       "   train_seasonal_strength_1y_7  \n",
       "0                      0.008107  \n",
       "1                      0.005128  \n",
       "2                      0.003238  \n",
       "3                      0.000967  \n",
       "4                      0.000275  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cyclical_strength(train_start, train_end, symbol, experiment_name, window_size, period, moving_avg_window=30):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the cyclical component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the cyclical strength.\n",
    "    period (int): The period of the cycle (e.g., 365 for annual cycles in daily data).\n",
    "    moving_avg_window (int): Window size for the moving average to approximate the cyclical component.\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized cyclical strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the residual component (contains cyclical + noise components)\n",
    "    residual = result.resid\n",
    "\n",
    "    # Apply moving average to the residuals to approximate the cyclical component\n",
    "    cyclical = residual.rolling(window=moving_avg_window, center=True).mean()\n",
    "\n",
    "    # Calculate variance of the cyclical component and the residuals, ignoring NaNs\n",
    "    cyclical_variance = np.nanvar(cyclical)\n",
    "    residual_variance = np.nanvar(residual - cyclical)  # Noise component variance\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (cyclical_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate normalized cyclical strength as a proportion of total variability\n",
    "    normalized_cyclical_strength = (cyclical_variance / (cyclical_variance + residual_variance)) / mean_of_series\n",
    "    \n",
    "    return normalized_cyclical_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_noise_strength(train_start, train_end, symbol, experiment_name, window_size, period):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the noise (residual) component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the noise strength.\n",
    "    period (int): The period of the seasonality (e.g., 365 for yearly seasonality on daily data).\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized noise strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the seasonal and residual components\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Calculate variance of seasonal and residual components, ignoring NaNs\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if mean_of_series == 0 or residual_variance == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate normalized noise strength\n",
    "    normalized_noise_strength = residual_variance / mean_of_series\n",
    "    \n",
    "    return normalized_noise_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_efficiency_ratio(train_start, train_end, symbol, experiment_name, window):\n",
    "    \"\"\"\n",
    "    Calculate the overall Efficiency Ratio (Fractal Efficiency) of a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    \n",
    "    Returns:\n",
    "    float: Efficiency Ratio for the entire series.\n",
    "    \"\"\"\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "        \n",
    "    # Calculate the Net Change over the entire series\n",
    "    net_change = abs(series_window.iloc[-1] - series_window.iloc[0])\n",
    "    \n",
    "    # Calculate the Total Change across all points in the series\n",
    "    total_change = series_window.diff().abs().sum()\n",
    "    \n",
    "    # Calculate the Efficiency Ratio\n",
    "    if total_change == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    \n",
    "    efficiency_ratio = net_change / total_change\n",
    "    return efficiency_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent_of_max(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    s_max = series.max()\n",
    "    perc_of_max = series.tail(1)[0]/s_max\n",
    "    \n",
    "    return perc_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_drawdown(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    \n",
    "    s = series.pct_change()\n",
    "    max_drawdown = qs.stats.max_drawdown(s)\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]/len(series)\n",
    "    \n",
    "    return max_drawdown, max_drawdown_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    \n",
    "    std = series.std()/series.mean()\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_period_without_trades(symbol, experiment_name, train_test_comb):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    df_trades['exit_ts_shift'] = df_trades['Exit Timestamp'].shift(1)\n",
    "    period_length = (df_trades['Exit Timestamp'].max() - df_trades['Entry Timestamp'].min()).days\n",
    "    df_trades['days_between_trade'] = ((df_trades['Entry Timestamp'] - df_trades['exit_ts_shift']).dt.days)/period_length\n",
    "    min_days = df_trades['days_between_trade'].min()\n",
    "    max_days = df_trades['days_between_trade'].max()\n",
    "    mean_days = df_trades['days_between_trade'].mean()\n",
    "    std_days = df_trades['days_between_trade'].std()    \n",
    "    \n",
    "    return min_days, max_days, mean_days, std_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trades_per_period(symbol, experiment_name, train_test_comb):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    df_trades.set_index('Entry Timestamp', inplace = True)\n",
    "    df_g = df_trades.resample('M')['PnL'].count()\n",
    "    min_trades = df_g.min()\n",
    "    max_trades = df_g.max()\n",
    "    mean_trades = df_g.mean()\n",
    "    std_trades = df_g.std()\n",
    "    return min_trades, max_trades, mean_trades, std_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trades_returns(symbol, experiment_name, train_test_comb):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    min_trades = df_trades['Return'].min()\n",
    "    max_trades = df_trades['Return'].max()\n",
    "    mean_trades = df_trades['Return'].mean()\n",
    "    std_trades = df_trades['Return'].std()\n",
    "    \n",
    "    return min_trades, max_trades, mean_trades, std_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_drawdown_period(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    \n",
    "    s = series.pct_change()\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]\n",
    "    current_drawdown_duration = (cummax == cummax.tail(1)[0]).sum()\n",
    "    return current_drawdown_duration/max_drawdown_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_returns_corr(train_start, train_end, symbol, experiment_name, train_test_comb):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    df = pd.concat([series, (returns + 1).cumprod()], axis = 1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace = True)\n",
    "    corr = df.corr().iloc[0,1]\n",
    "    return corr, df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_calpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
