{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import quantstats as qs\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/df_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2006-06-19 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2009-07-17 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2012-08-08 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2015-09-15 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2018-08-29 00:00:00</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2021-07-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>End</td>\n",
       "      <td>2006-06-16 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>2024-10-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Period</td>\n",
       "      <td>1618 days 00:00:00</td>\n",
       "      <td>775 days 00:00:00</td>\n",
       "      <td>2393 days 00:00:00</td>\n",
       "      <td>772 days 00:00:00</td>\n",
       "      <td>3165 days 00:00:00</td>\n",
       "      <td>779 days 00:00:00</td>\n",
       "      <td>3944 days 00:00:00</td>\n",
       "      <td>745 days 00:00:00</td>\n",
       "      <td>4689 days 00:00:00</td>\n",
       "      <td>721 days 00:00:00</td>\n",
       "      <td>5410 days 00:00:00</td>\n",
       "      <td>816 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Start Value</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>End Value</td>\n",
       "      <td>290.77508190136484</td>\n",
       "      <td>242.1327157507496</td>\n",
       "      <td>3209.085046269844</td>\n",
       "      <td>306.1137161032776</td>\n",
       "      <td>3462.3753343410394</td>\n",
       "      <td>240.99195732631208</td>\n",
       "      <td>6015.343803012408</td>\n",
       "      <td>211.483926360233</td>\n",
       "      <td>8837.467898317822</td>\n",
       "      <td>230.3408712190465</td>\n",
       "      <td>88960.23579451621</td>\n",
       "      <td>546.3912384357037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                    0                    1                    2  \\\n",
       "0        Start  2000-01-10 00:00:00  2006-06-19 00:00:00  2000-01-10 00:00:00   \n",
       "1          End  2006-06-16 00:00:00  2009-07-16 00:00:00  2009-07-16 00:00:00   \n",
       "2       Period   1618 days 00:00:00    775 days 00:00:00   2393 days 00:00:00   \n",
       "3  Start Value                100.0                100.0                100.0   \n",
       "4    End Value   290.77508190136484    242.1327157507496    3209.085046269844   \n",
       "\n",
       "                     3                    4                    5  \\\n",
       "0  2009-07-17 00:00:00  2000-01-10 00:00:00  2012-08-08 00:00:00   \n",
       "1  2012-08-07 00:00:00  2012-08-07 00:00:00  2015-09-14 00:00:00   \n",
       "2    772 days 00:00:00   3165 days 00:00:00    779 days 00:00:00   \n",
       "3                100.0                100.0                100.0   \n",
       "4    306.1137161032776   3462.3753343410394   240.99195732631208   \n",
       "\n",
       "                     6                    7                    8  \\\n",
       "0  2000-01-10 00:00:00  2015-09-15 00:00:00  2000-01-10 00:00:00   \n",
       "1  2015-09-14 00:00:00  2018-08-28 00:00:00  2018-08-28 00:00:00   \n",
       "2   3944 days 00:00:00    745 days 00:00:00   4689 days 00:00:00   \n",
       "3                100.0                100.0                100.0   \n",
       "4    6015.343803012408     211.483926360233    8837.467898317822   \n",
       "\n",
       "                     9                   10                   11  \n",
       "0  2018-08-29 00:00:00  2000-01-10 00:00:00  2021-07-15 00:00:00  \n",
       "1  2021-07-12 00:00:00  2021-07-12 00:00:00  2024-10-10 00:00:00  \n",
       "2    721 days 00:00:00   5410 days 00:00:00    816 days 00:00:00  \n",
       "3                100.0                100.0                100.0  \n",
       "4    230.3408712190465    88960.23579451621    546.3912384357037  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# existing stats from pf.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>train_end_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_expectancy</th>\n",
       "      <th>test_expectancy</th>\n",
       "      <th>train_sharpe_ratio</th>\n",
       "      <th>test_sharpe_ratio</th>\n",
       "      <th>train_calmar_ratio</th>\n",
       "      <th>test_calmar_ratio</th>\n",
       "      <th>train_omega_ratio</th>\n",
       "      <th>test_omega_ratio</th>\n",
       "      <th>train_sortino_ratio</th>\n",
       "      <th>test_sortino_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2006-06-19 00:00:00</td>\n",
       "      <td>2006-06-16 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>290.775082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.295368</td>\n",
       "      <td>3.384112</td>\n",
       "      <td>0.975684</td>\n",
       "      <td>0.980142</td>\n",
       "      <td>0.426440</td>\n",
       "      <td>1.747438</td>\n",
       "      <td>1.250014</td>\n",
       "      <td>1.329824</td>\n",
       "      <td>1.565967</td>\n",
       "      <td>1.576195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2009-07-17 00:00:00</td>\n",
       "      <td>2009-07-16 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2393.0</td>\n",
       "      <td>772.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3209.085046</td>\n",
       "      <td>...</td>\n",
       "      <td>22.932115</td>\n",
       "      <td>4.528574</td>\n",
       "      <td>1.106845</td>\n",
       "      <td>1.087510</td>\n",
       "      <td>1.464769</td>\n",
       "      <td>2.977417</td>\n",
       "      <td>1.300569</td>\n",
       "      <td>1.300668</td>\n",
       "      <td>1.773852</td>\n",
       "      <td>1.705708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2012-08-08 00:00:00</td>\n",
       "      <td>2012-08-07 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>3165.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3462.375334</td>\n",
       "      <td>...</td>\n",
       "      <td>17.675559</td>\n",
       "      <td>2.660226</td>\n",
       "      <td>1.027141</td>\n",
       "      <td>0.934371</td>\n",
       "      <td>0.643946</td>\n",
       "      <td>2.646347</td>\n",
       "      <td>1.278662</td>\n",
       "      <td>1.269234</td>\n",
       "      <td>1.646453</td>\n",
       "      <td>1.471992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2015-09-15 00:00:00</td>\n",
       "      <td>2015-09-14 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>3944.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6015.343803</td>\n",
       "      <td>...</td>\n",
       "      <td>20.183980</td>\n",
       "      <td>1.548388</td>\n",
       "      <td>0.992068</td>\n",
       "      <td>0.865236</td>\n",
       "      <td>0.711761</td>\n",
       "      <td>2.601962</td>\n",
       "      <td>1.268213</td>\n",
       "      <td>1.203659</td>\n",
       "      <td>1.569741</td>\n",
       "      <td>1.292896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2018-08-29 00:00:00</td>\n",
       "      <td>2018-08-28 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>4689.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8837.467898</td>\n",
       "      <td>...</td>\n",
       "      <td>28.689557</td>\n",
       "      <td>1.780570</td>\n",
       "      <td>0.939279</td>\n",
       "      <td>0.958932</td>\n",
       "      <td>0.607345</td>\n",
       "      <td>3.048036</td>\n",
       "      <td>1.259467</td>\n",
       "      <td>1.221953</td>\n",
       "      <td>1.485741</td>\n",
       "      <td>1.477511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-01-10 00:00:00</td>\n",
       "      <td>2021-07-15 00:00:00</td>\n",
       "      <td>2021-07-12 00:00:00</td>\n",
       "      <td>2024-10-10 00:00:00</td>\n",
       "      <td>5410.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88960.235795</td>\n",
       "      <td>...</td>\n",
       "      <td>223.869176</td>\n",
       "      <td>7.262950</td>\n",
       "      <td>0.987662</td>\n",
       "      <td>2.365588</td>\n",
       "      <td>1.082136</td>\n",
       "      <td>7.418604</td>\n",
       "      <td>1.281594</td>\n",
       "      <td>1.646318</td>\n",
       "      <td>1.568137</td>\n",
       "      <td>4.174921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_test_combination          train_start           test_start  \\\n",
       "0                     0.0  2000-01-10 00:00:00  2006-06-19 00:00:00   \n",
       "1                     1.0  2000-01-10 00:00:00  2009-07-17 00:00:00   \n",
       "2                     2.0  2000-01-10 00:00:00  2012-08-08 00:00:00   \n",
       "3                     3.0  2000-01-10 00:00:00  2015-09-15 00:00:00   \n",
       "4                     4.0  2000-01-10 00:00:00  2018-08-29 00:00:00   \n",
       "5                     5.0  2000-01-10 00:00:00  2021-07-15 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-06-16 00:00:00  2009-07-16 00:00:00        1618.0        775.0   \n",
       "1  2009-07-16 00:00:00  2012-08-07 00:00:00        2393.0        772.0   \n",
       "2  2012-08-07 00:00:00  2015-09-14 00:00:00        3165.0        779.0   \n",
       "3  2015-09-14 00:00:00  2018-08-28 00:00:00        3944.0        745.0   \n",
       "4  2018-08-28 00:00:00  2021-07-12 00:00:00        4689.0        721.0   \n",
       "5  2021-07-12 00:00:00  2024-10-10 00:00:00        5410.0        816.0   \n",
       "\n",
       "   train_start_value  test_start_value  train_end_value  ...  \\\n",
       "0              100.0             100.0       290.775082  ...   \n",
       "1              100.0             100.0      3209.085046  ...   \n",
       "2              100.0             100.0      3462.375334  ...   \n",
       "3              100.0             100.0      6015.343803  ...   \n",
       "4              100.0             100.0      8837.467898  ...   \n",
       "5              100.0             100.0     88960.235795  ...   \n",
       "\n",
       "   train_expectancy  test_expectancy  train_sharpe_ratio  test_sharpe_ratio  \\\n",
       "0          1.295368         3.384112            0.975684           0.980142   \n",
       "1         22.932115         4.528574            1.106845           1.087510   \n",
       "2         17.675559         2.660226            1.027141           0.934371   \n",
       "3         20.183980         1.548388            0.992068           0.865236   \n",
       "4         28.689557         1.780570            0.939279           0.958932   \n",
       "5        223.869176         7.262950            0.987662           2.365588   \n",
       "\n",
       "   train_calmar_ratio  test_calmar_ratio  train_omega_ratio  test_omega_ratio  \\\n",
       "0            0.426440           1.747438           1.250014          1.329824   \n",
       "1            1.464769           2.977417           1.300569          1.300668   \n",
       "2            0.643946           2.646347           1.278662          1.269234   \n",
       "3            0.711761           2.601962           1.268213          1.203659   \n",
       "4            0.607345           3.048036           1.259467          1.221953   \n",
       "5            1.082136           7.418604           1.281594          1.646318   \n",
       "\n",
       "   train_sortino_ratio  test_sortino_ratio  \n",
       "0             1.565967            1.576195  \n",
       "1             1.773852            1.705708  \n",
       "2             1.646453            1.471992  \n",
       "3             1.569741            1.292896  \n",
       "4             1.485741            1.477511  \n",
       "5             1.568137            4.174921  \n",
       "\n",
       "[6 rows x 57 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_existing_stats(df_stats):\n",
    "    df_stats = df_stats.set_index('Unnamed: 0')\n",
    "    df_stats.index.name = None\n",
    "    \n",
    "    transformed_rows = []\n",
    "\n",
    "    # Iterate over pairs of columns (train-test combinations)\n",
    "    for i in range(0, df_stats.shape[1], 2):\n",
    "        train_col = i\n",
    "        test_col = i + 1\n",
    "        \n",
    "        # Create a dictionary to store values for the current combination\n",
    "        row = {}\n",
    "        \n",
    "        # Set the combination number\n",
    "        row['train_test_combination'] = i // 2\n",
    "        \n",
    "        # Add train and test statistics to the row with new column names\n",
    "        for stat in df_stats.index:\n",
    "            row[f'train_{stat.lower().replace(\" \", \"_\")}'] = df_stats.loc[stat, str(train_col)]\n",
    "            row[f'test_{stat.lower().replace(\" \", \"_\")}'] = df_stats.loc[stat, str(test_col)]\n",
    "        \n",
    "        # Append the row to the transformed rows list\n",
    "        transformed_rows.append(row)\n",
    "\n",
    "    # Create a new DataFrame from the transformed rows\n",
    "    df_transformed = pd.DataFrame(transformed_rows)\n",
    "    excluded_columns = {'train_start', 'test_start', 'train_end', 'test_end', 'train_period', 'test_period'}\n",
    "    for col in df_transformed.columns:\n",
    "        if 'duration' not in col and col not in excluded_columns:\n",
    "            df_transformed[col] = df_transformed[col].astype(float)\n",
    "        elif (col in excluded_columns and 'period' in col) or ('duration' in col):\n",
    "            # Convert period columns to float days\n",
    "            df_transformed[col] = pd.to_timedelta(df_transformed[col]).dt.total_seconds() / (24 * 3600)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "process_existing_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trend strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Creating a sample time series data (you can replace this with your stock data)\n",
    "dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')\n",
    "np.random.seed(42)\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "\n",
    "# Create a DataFrame with the dates and prices\n",
    "df_test = pd.DataFrame({'Date': dates, 'Price': price_data})\n",
    "df_test.set_index('Date', inplace=True)\n",
    "\n",
    "# Function to calculate the trend slope over the last 'window_size' observations\n",
    "def calculate_trend_slope(train_start, train_end, test_start, test_end, window_size):\n",
    "    series = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/df_price.csv', index = 'Date')['Adj Close']\n",
    "    if len(series) < window_size:\n",
    "        raise ValueError(f\"Series length is less than the window size {window_size}\")\n",
    "    \n",
    "    # Extract the last 'window_size' observations\n",
    "    y = series[-window_size:]\n",
    "    x = np.arange(len(y))  # Time indices\n",
    "\n",
    "    # Perform linear regression to calculate slope\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "    return slope\n",
    "\n",
    "# # Calculate the slope for different window sizes\n",
    "# window_50 = calculate_trend_slope(df_test['Price'], window_size=50)\n",
    "# window_200 = calculate_trend_slope(df_test['Price'], window_size=200)\n",
    "\n",
    "# print(f\"Trend slope for last 50 observations: {window_50}\")\n",
    "# print(f\"Trend slope for last 200 observations: {window_200}\")\n",
    "\n",
    "# # Plot the data and trend line\n",
    "# plt.plot(df_test.index, df_test['Price'], label='Price')\n",
    "\n",
    "# # Overlay the trend line for the last 50 observations\n",
    "# x_50 = np.arange(50)\n",
    "# y_trend_50 = window_50 * x_50 + df_test['Price'][-50].mean()  # Adjust to match the series' mean\n",
    "# plt.plot(df_test.index[-50:], y_trend_50, color='red', label='Trend (last 50)')\n",
    "\n",
    "# # Overlay the trend line for the last 200 observations\n",
    "# x_200 = np.arange(200)\n",
    "# y_trend_200 = window_200 * x_200 + df_test['Price'][-200].mean()  # Adjust to match the series' mean\n",
    "# plt.plot(df_test.index[-200:], y_trend_200, color='green', label='Trend (last 200)')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seasonal strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal strength for the last 200 days: 0.0016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def calculate_seasonal_strength(series, window_size, period):\n",
    "    \n",
    "    # Get the last 'window_size' observations from the series\n",
    "    series_window = series.tail(window_size)\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the seasonal and residual components\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Calculate variance of seasonal and residual components, ignoring NaNs\n",
    "    seasonal_variance = np.nanvar(seasonal)\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    \n",
    "    # Normalize by the mean or standard deviation of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (seasonal_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate normalized seasonal strength: seasonal_variance / residual_variance, normalized by the mean of the series\n",
    "    normalized_seasonal_strength = seasonal_variance / (seasonal_variance + residual_variance) / mean_of_series # maybe remove (/s var + res var)\n",
    "    \n",
    "    return normalized_seasonal_strength\n",
    "\n",
    "\n",
    "\n",
    "# Example: Generating sample time series data (replace this with actual stock data)\n",
    "dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')\n",
    "np.random.seed(42)\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "\n",
    "# Create a DataFrame with the dates and prices\n",
    "df_test = pd.DataFrame({'Date': dates, 'Price': price_data})\n",
    "df_test.set_index('Date', inplace=True)\n",
    "\n",
    "# Use the function to calculate seasonal strength for the last 200 days\n",
    "window_size = 200\n",
    "period = 30  # Monthly seasonality, 7 weekly, 365 yearly\n",
    "\n",
    "seasonal_strength_value = calculate_seasonal_strength(df_test['Price'], window_size, period)\n",
    "\n",
    "print(f\"Seasonal strength for the last {window_size} days: {seasonal_strength_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cyclical strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclical Strength: 0.019642711876708036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def calculate_cyclical_strength(series, window_size, period, moving_avg_window=30):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the cyclical component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the cyclical strength.\n",
    "    period (int): The period of the cycle (e.g., 365 for annual cycles in daily data).\n",
    "    moving_avg_window (int): Window size for the moving average to approximate the cyclical component.\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized cyclical strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the last 'window_size' observations from the series\n",
    "    series_window = pd.Series(series).tail(window_size)\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the residual component (contains cyclical + noise components)\n",
    "    residual = result.resid\n",
    "\n",
    "    # Apply moving average to the residuals to approximate the cyclical component\n",
    "    cyclical = residual.rolling(window=moving_avg_window, center=True).mean()\n",
    "\n",
    "    # Calculate variance of the cyclical component and the residuals, ignoring NaNs\n",
    "    cyclical_variance = np.nanvar(cyclical)\n",
    "    residual_variance = np.nanvar(residual - cyclical)  # Noise component variance\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (cyclical_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate normalized cyclical strength as a proportion of total variability\n",
    "    normalized_cyclical_strength = (cyclical_variance / (cyclical_variance + residual_variance)) / mean_of_series\n",
    "    \n",
    "    return normalized_cyclical_strength\n",
    "\n",
    "\n",
    "# Example usage with synthetic data (assuming 'series' and 'period' are predefined)\n",
    "period = 365  # Assuming yearly seasonality for daily data\n",
    "window = 30  # The window size for the moving average to remove short-term noise\n",
    "window_size = 1000\n",
    "cyclical_strength_value = calculate_cyclical_strength(price_data, window_size, period, window)\n",
    "\n",
    "print(f\"Cyclical Strength: {cyclical_strength_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noise strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise strength for the last 200 days: 0.0484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def calculate_noise_strength(series, window_size, period):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the noise (residual) component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the noise strength.\n",
    "    period (int): The period of the seasonality (e.g., 365 for yearly seasonality on daily data).\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized noise strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the last 'window_size' observations from the series\n",
    "    series_window = series.tail(window_size)\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the seasonal and residual components\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Calculate variance of seasonal and residual components, ignoring NaNs\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if mean_of_series == 0 or residual_variance == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate normalized noise strength\n",
    "    normalized_noise_strength = residual_variance / mean_of_series\n",
    "    \n",
    "    return normalized_noise_strength\n",
    "\n",
    "# Example usage: \n",
    "dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')\n",
    "np.random.seed(42)\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "\n",
    "df_test = pd.DataFrame({'Date': dates, 'Price': price_data})\n",
    "df_test.set_index('Date', inplace=True)\n",
    "\n",
    "window_size = 200\n",
    "period = 30  # Set an appropriate period based on data frequency\n",
    "\n",
    "noise_strength_value = calculate_noise_strength(df_test['Price'], window_size, period)\n",
    "print(f\"Noise strength for the last {window_size} days: {noise_strength_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fractal efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Efficiency Ratio: 0.0084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_overall_efficiency_ratio(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the overall Efficiency Ratio (Fractal Efficiency) of a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    \n",
    "    Returns:\n",
    "    float: Efficiency Ratio for the entire series.\n",
    "    \"\"\"\n",
    "    series_window = pd.Series(series).tail(window)\n",
    "    # Calculate the Net Change over the entire series\n",
    "    net_change = abs(series_window.iloc[-1] - series_window.iloc[0])\n",
    "    \n",
    "    # Calculate the Total Change across all points in the series\n",
    "    total_change = series_window.diff().abs().sum()\n",
    "    \n",
    "    # Calculate the Efficiency Ratio\n",
    "    if total_change == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    \n",
    "    efficiency_ratio = net_change / total_change\n",
    "    return efficiency_ratio\n",
    "\n",
    "# Example usage\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "# Calculate the Efficiency Ratio for the entire series\n",
    "overall_efficiency_ratio = calculate_overall_efficiency_ratio(series, 50)\n",
    "print(f\"Overall Efficiency Ratio: {overall_efficiency_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# current price as % of max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last price percent of overall max price: 0.9063\n"
     ]
    }
   ],
   "source": [
    "def calculate_percent_of_max(series):\n",
    "    s_max = series.max()\n",
    "    perc_of_max = series.tail(1)[0]/s_max\n",
    "    \n",
    "    return perc_of_max\n",
    "\n",
    "# Example usage\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "perc_of_max = calculate_percent_of_max(series)\n",
    "print(f\"Last price percent of overall max price: {perc_of_max:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max drawdown & max drawdown duration of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price max drawdown: -0.1392\n",
      "Price max drawdown duration as % of whole period: 0.61\n"
     ]
    }
   ],
   "source": [
    "def calculate_max_drawdown(series):\n",
    "    s = series.pct_change()\n",
    "    max_drawdown = qs.stats.max_drawdown(s)\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]/len(series)\n",
    "    \n",
    "    return max_drawdown, max_drawdown_duration\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "max_d, max_d_dur = calculate_max_drawdown(series)\n",
    "print(f\"Price max drawdown: {max_d:.4f}\")\n",
    "print(f\"Price max drawdown duration as % of whole period: {max_d_dur}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standard deviation of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price standard deviation: 0.0843\n"
     ]
    }
   ],
   "source": [
    "def calculate_std(series):\n",
    "    std = series.std()/series.mean()\n",
    "    return std\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "std = calculate_std(series)\n",
    "print(f\"Price standard deviation: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc stats of number of days between trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.00043687199650502403, 0.027959807776321538, 0.004621645857763674, 0.006057585335997416)\n"
     ]
    }
   ],
   "source": [
    "def calculate_period_without_trades(df_trades):\n",
    "    df_trades['exit_ts_shift'] = df_trades['Exit Timestamp'].shift(1)\n",
    "    period_length = (df_trades['Exit Timestamp'].max() - df_trades['Entry Timestamp'].min()).days\n",
    "    df_trades['days_between_trade'] = ((df_trades['Entry Timestamp'] - df_trades['exit_ts_shift']).dt.days)/period_length\n",
    "    min_days = df_trades['days_between_trade'].min()\n",
    "    max_days = df_trades['days_between_trade'].max()\n",
    "    mean_days = df_trades['days_between_trade'].mean()\n",
    "    std_days = df_trades['days_between_trade'].std()    \n",
    "    \n",
    "    return min_days, max_days, mean_days, std_days\n",
    "\n",
    "df_trades = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/train_0_trades.csv')\n",
    "df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "desc_days_trades = calculate_period_without_trades(df_trades)\n",
    "print(desc_days_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc of trades per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 1.763157894736842, 1.220583697008734)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_trades_per_period(df_trades):\n",
    "    df_trades.set_index('Entry Timestamp', inplace = True)\n",
    "    df_g = df_trades.resample('M')['PnL'].count()\n",
    "    min_trades = df_g.min()\n",
    "    max_trades = df_g.max()\n",
    "    mean_trades = df_g.mean()\n",
    "    std_trades = df_g.std()\n",
    "    return min_trades, max_trades, mean_trades, std_trades\n",
    "\n",
    "df_trades = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/train_0_trades.csv')\n",
    "df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "calculate_trades_per_period(df_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc of trades returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2761519761776835,\n",
       " 0.2499999999999999,\n",
       " 0.011366545607989977,\n",
       " 0.07980345577870791)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_trades_returns(df_trades):\n",
    "    min_trades = df_trades['Return'].min()\n",
    "    max_trades = df_trades['Return'].max()\n",
    "    mean_trades = df_trades['Return'].mean()\n",
    "    std_trades = df_trades['Return'].std()\n",
    "    \n",
    "    return min_trades, max_trades, mean_trades, std_trades\n",
    "\n",
    "df_trades = pd.read_csv('../outputs_hhhl_ml1/before_good26_rest/ORCL/backtest/train_0_trades.csv')\n",
    "df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "calculate_trades_returns(df_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perc of current drawdown period from max drawdown period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perc_drawdown_period(series):\n",
    "    s = series.pct_change()\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]\n",
    "    current_drawdown_duration = (cummax == cummax.tail(1)[0]).sum()\n",
    "    return current_drawdown_duration/max_drawdown_duration\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "\n",
    "perc_drawdown_duration = calculate_perc_drawdown_period(series)\n",
    "perc_drawdown_duration\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation of cumulative returns with price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4697479615044177"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_price_returns_corr(series, df_trades):\n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    df = pd.concat([series, (returns + 1).cumprod()], axis = 1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace = True)\n",
    "    corr = df.corr().iloc[0,1]\n",
    "    return corr, df\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "df_trades = pd.DataFrame({'Return': np.random.rand(len(dates)), 'Exit Timestamp': dates})\n",
    "\n",
    "corr, df_out = calculate_price_returns_corr(series, df_trades)\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# desc stats of rolling correlation of returns and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7456334463291172,\n",
       " 0.9526217962634782,\n",
       " 0.3283485567919696,\n",
       " 0.5163894125931645)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_price_returns_rolling_corr(series, df_trades, window):\n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    df = pd.concat([series, (returns + 1).cumprod()], axis = 1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace = True)\n",
    "    price = df.iloc[:, 0]\n",
    "    trades_returns = df.iloc[:, 1]\n",
    "    rolling_corr = price.rolling(window = window).corr(trades_returns).dropna()\n",
    "    min_corr = rolling_corr.min()\n",
    "    max_corr = rolling_corr.max()\n",
    "    mean_corr = rolling_corr.mean()\n",
    "    std_corr = rolling_corr.std()\n",
    "    return min_corr, max_corr, mean_corr, std_corr\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "df_trades = pd.DataFrame({'Return': np.random.rand(len(dates)), 'Exit Timestamp': dates})\n",
    "\n",
    "t = calculate_price_returns_rolling_corr(series, df_trades, 10)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outlier counts in trades returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_outlier_cnts(df_trades, threshold):\n",
    "    outliers_cnt = (df_trades['Return'] <= threshold).sum()\n",
    "    return outliers_cnt/len(df_trades)\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "price_data = 50 + np.random.normal(0, 1, len(dates)).cumsum()\n",
    "series = pd.Series(price_data, index=dates)\n",
    "df_trades = pd.DataFrame({'Return': np.random.rand(len(dates)), 'Exit Timestamp': dates})\n",
    "\n",
    "calculate_outlier_cnts(df_trades, -0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model train stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'False_precision': 0.99,\n",
       " 'False_recall': 0.97,\n",
       " 'False_f1_score': 0.98,\n",
       " 'True_precision': 0.94,\n",
       " 'True_recall': 0.98,\n",
       " 'True_f1_score': 0.96,\n",
       " 'accuracy': 0.97,\n",
       " 'macro avg_precision': 0.96,\n",
       " 'macro avg_recall': 0.97,\n",
       " 'macro avg_f1_score': 0.97,\n",
       " 'weighted avg_precision': 0.97,\n",
       " 'weighted avg_recall': 0.97,\n",
       " 'weighted avg_f1_score': 0.97}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_class_report(report):\n",
    "    pattern = r'(\\b(?:False|True|accuracy|macro avg|weighted avg)\\b)\\s+([\\d.]*)\\s+([\\d.]+)\\s+([\\d.]+)'\n",
    "    matches = re.findall(pattern, report)\n",
    "\n",
    "    # Create dictionary to store metrics\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for match in matches:\n",
    "        label, precision, recall, f1_score = match\n",
    "        if label == 'accuracy':  # Only one value for accuracy\n",
    "            metrics_dict[label] = float(recall)\n",
    "        else:\n",
    "            if precision:  # Ignore empty precision values for rows without it\n",
    "                metrics_dict[f\"{label}_precision\"] = float(precision)\n",
    "            metrics_dict[f\"{label}_recall\"] = float(recall)\n",
    "            metrics_dict[f\"{label}_f1_score\"] = float(f1_score)\n",
    "    return metrics_dict\n",
    "train_stuff = pickle.load(open('../outputs_hhhl_ml1/before_good26_rest/ORCL/train_stuff.pickle', 'rb'))\n",
    "report = train_stuff['train_class_reports'][0]\n",
    "get_class_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'before_good26_rest'\n",
    "experiment_path = f\"../outputs_hhhl_ml1/{experiment_name}\"\n",
    "\n",
    "symbols = [name for name in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_symbols_dfs = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    symbol_path = f\"{experiment_path}/{symbol}\"\n",
    "    df_stats = pd.read_csv(f\"{symbol_path}/backtest/df_stats.csv\")\n",
    "    df_symbol = process_existing_stats(df_stats)\n",
    "    df_symbol['symbol'] = symbol\n",
    "    colnames = [df_symbol.columns[-1]] + list(df_symbol.columns[:-1])\n",
    "    df_symbol = df_symbol[colnames]\n",
    "    list_symbols_dfs.append(df_symbol)\n",
    "df = pd.concat(list_symbols_dfs, axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1052, 58)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_expectancy</th>\n",
       "      <th>test_expectancy</th>\n",
       "      <th>train_sharpe_ratio</th>\n",
       "      <th>test_sharpe_ratio</th>\n",
       "      <th>train_calmar_ratio</th>\n",
       "      <th>test_calmar_ratio</th>\n",
       "      <th>train_omega_ratio</th>\n",
       "      <th>test_omega_ratio</th>\n",
       "      <th>train_sortino_ratio</th>\n",
       "      <th>test_sortino_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.654207</td>\n",
       "      <td>2.855433</td>\n",
       "      <td>1.274314</td>\n",
       "      <td>0.657814</td>\n",
       "      <td>1.879271</td>\n",
       "      <td>0.933827</td>\n",
       "      <td>1.343739</td>\n",
       "      <td>1.337054</td>\n",
       "      <td>2.098648</td>\n",
       "      <td>1.044396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.243531</td>\n",
       "      <td>2.866706</td>\n",
       "      <td>1.220968</td>\n",
       "      <td>1.141061</td>\n",
       "      <td>2.072756</td>\n",
       "      <td>3.707172</td>\n",
       "      <td>1.339096</td>\n",
       "      <td>1.280292</td>\n",
       "      <td>1.997439</td>\n",
       "      <td>1.792124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.706777</td>\n",
       "      <td>0.869280</td>\n",
       "      <td>1.178688</td>\n",
       "      <td>0.747883</td>\n",
       "      <td>2.326982</td>\n",
       "      <td>1.059748</td>\n",
       "      <td>1.334222</td>\n",
       "      <td>1.176476</td>\n",
       "      <td>1.927009</td>\n",
       "      <td>1.124046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>137.488440</td>\n",
       "      <td>1.802699</td>\n",
       "      <td>1.130173</td>\n",
       "      <td>0.751539</td>\n",
       "      <td>2.269650</td>\n",
       "      <td>2.176509</td>\n",
       "      <td>1.326110</td>\n",
       "      <td>1.187576</td>\n",
       "      <td>1.839687</td>\n",
       "      <td>1.130974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>169.810563</td>\n",
       "      <td>1.601596</td>\n",
       "      <td>1.048429</td>\n",
       "      <td>0.921876</td>\n",
       "      <td>1.858248</td>\n",
       "      <td>2.395654</td>\n",
       "      <td>1.312682</td>\n",
       "      <td>1.179903</td>\n",
       "      <td>1.698057</td>\n",
       "      <td>1.374492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TJX</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2021-12-28 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>2024-10-17 00:00:00</td>\n",
       "      <td>5528.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>409.687788</td>\n",
       "      <td>1.725069</td>\n",
       "      <td>1.025770</td>\n",
       "      <td>1.419325</td>\n",
       "      <td>1.954004</td>\n",
       "      <td>3.024418</td>\n",
       "      <td>1.310984</td>\n",
       "      <td>1.335233</td>\n",
       "      <td>1.667681</td>\n",
       "      <td>2.168212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-02-06 00:00:00</td>\n",
       "      <td>2018-08-02 00:00:00</td>\n",
       "      <td>2018-08-01 00:00:00</td>\n",
       "      <td>2021-05-28 00:00:00</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.517477</td>\n",
       "      <td>2.053002</td>\n",
       "      <td>1.052592</td>\n",
       "      <td>1.298221</td>\n",
       "      <td>2.405298</td>\n",
       "      <td>4.799122</td>\n",
       "      <td>1.238280</td>\n",
       "      <td>1.334131</td>\n",
       "      <td>1.570554</td>\n",
       "      <td>2.099449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-02-06 00:00:00</td>\n",
       "      <td>2021-06-01 00:00:00</td>\n",
       "      <td>2021-05-28 00:00:00</td>\n",
       "      <td>2024-10-17 00:00:00</td>\n",
       "      <td>2093.0</td>\n",
       "      <td>852.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.895038</td>\n",
       "      <td>2.595950</td>\n",
       "      <td>1.172641</td>\n",
       "      <td>1.889017</td>\n",
       "      <td>2.529929</td>\n",
       "      <td>4.206680</td>\n",
       "      <td>1.262204</td>\n",
       "      <td>1.427613</td>\n",
       "      <td>1.755537</td>\n",
       "      <td>3.193254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CSX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2006-08-03 00:00:00</td>\n",
       "      <td>2009-11-09 00:00:00</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>823.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.738321</td>\n",
       "      <td>6.089717</td>\n",
       "      <td>1.499536</td>\n",
       "      <td>1.592117</td>\n",
       "      <td>2.247327</td>\n",
       "      <td>2.529441</td>\n",
       "      <td>1.475021</td>\n",
       "      <td>1.383238</td>\n",
       "      <td>2.634410</td>\n",
       "      <td>2.648810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CSX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2009-11-10 00:00:00</td>\n",
       "      <td>2009-11-09 00:00:00</td>\n",
       "      <td>2012-11-27 00:00:00</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.785562</td>\n",
       "      <td>4.555247</td>\n",
       "      <td>1.411191</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>1.532272</td>\n",
       "      <td>2.078037</td>\n",
       "      <td>1.444509</td>\n",
       "      <td>1.362127</td>\n",
       "      <td>2.447735</td>\n",
       "      <td>1.617255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "5    TJX                     5.0  2000-01-07 00:00:00  2021-12-28 00:00:00   \n",
       "0    ZTS                     0.0  2013-02-06 00:00:00  2018-08-02 00:00:00   \n",
       "1    ZTS                     1.0  2013-02-06 00:00:00  2021-06-01 00:00:00   \n",
       "0    CSX                     0.0  2000-01-07 00:00:00  2006-08-04 00:00:00   \n",
       "1    CSX                     1.0  2000-01-07 00:00:00  2009-11-10 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "5  2021-12-27 00:00:00  2024-10-17 00:00:00        5528.0        706.0   \n",
       "0  2018-08-01 00:00:00  2021-05-28 00:00:00        1382.0        711.0   \n",
       "1  2021-05-28 00:00:00  2024-10-17 00:00:00        2093.0        852.0   \n",
       "0  2006-08-03 00:00:00  2009-11-09 00:00:00        1652.0        823.0   \n",
       "1  2009-11-09 00:00:00  2012-11-27 00:00:00        2475.0        767.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_expectancy  \\\n",
       "0              100.0             100.0  ...          6.654207   \n",
       "1              100.0             100.0  ...         16.243531   \n",
       "2              100.0             100.0  ...         56.706777   \n",
       "3              100.0             100.0  ...        137.488440   \n",
       "4              100.0             100.0  ...        169.810563   \n",
       "5              100.0             100.0  ...        409.687788   \n",
       "0              100.0             100.0  ...          3.517477   \n",
       "1              100.0             100.0  ...          6.895038   \n",
       "0              100.0             100.0  ...         13.738321   \n",
       "1              100.0             100.0  ...         45.785562   \n",
       "\n",
       "   test_expectancy  train_sharpe_ratio  test_sharpe_ratio  train_calmar_ratio  \\\n",
       "0         2.855433            1.274314           0.657814            1.879271   \n",
       "1         2.866706            1.220968           1.141061            2.072756   \n",
       "2         0.869280            1.178688           0.747883            2.326982   \n",
       "3         1.802699            1.130173           0.751539            2.269650   \n",
       "4         1.601596            1.048429           0.921876            1.858248   \n",
       "5         1.725069            1.025770           1.419325            1.954004   \n",
       "0         2.053002            1.052592           1.298221            2.405298   \n",
       "1         2.595950            1.172641           1.889017            2.529929   \n",
       "0         6.089717            1.499536           1.592117            2.247327   \n",
       "1         4.555247            1.411191           0.994682            1.532272   \n",
       "\n",
       "   test_calmar_ratio  train_omega_ratio  test_omega_ratio  \\\n",
       "0           0.933827           1.343739          1.337054   \n",
       "1           3.707172           1.339096          1.280292   \n",
       "2           1.059748           1.334222          1.176476   \n",
       "3           2.176509           1.326110          1.187576   \n",
       "4           2.395654           1.312682          1.179903   \n",
       "5           3.024418           1.310984          1.335233   \n",
       "0           4.799122           1.238280          1.334131   \n",
       "1           4.206680           1.262204          1.427613   \n",
       "0           2.529441           1.475021          1.383238   \n",
       "1           2.078037           1.444509          1.362127   \n",
       "\n",
       "   train_sortino_ratio  test_sortino_ratio  \n",
       "0             2.098648            1.044396  \n",
       "1             1.997439            1.792124  \n",
       "2             1.927009            1.124046  \n",
       "3             1.839687            1.130974  \n",
       "4             1.698057            1.374492  \n",
       "5             1.667681            2.168212  \n",
       "0             1.570554            2.099449  \n",
       "1             1.755537            3.193254  \n",
       "0             2.634410            2.648810  \n",
       "1             2.447735            1.617255  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend_slope(train_start, train_end, symbol, experiment_name, window_size):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if len(series) < window_size:\n",
    "        raise ValueError(f\"Series length is less than the window size {window_size}\")\n",
    "    \n",
    "    # Extract the last 'window_size' observations\n",
    "    y = series[-window_size:]\n",
    "    x = np.arange(len(y))  # Time indices\n",
    "\n",
    "    # Perform linear regression to calculate slope\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "    return slope\n",
    "df['train_slope_250'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 250), axis = 'columns')\n",
    "df['train_slope_100'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 100), axis = 'columns')\n",
    "df['train_slope_50'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 50), axis = 'columns')\n",
    "df['train_slope_20'] = df.apply(lambda row:calculate_trend_slope(row['train_start'], row['train_end'],\n",
    "                                          row['symbol'], experiment_name, 20), axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seasonal_strength(train_start, train_end, symbol, experiment_name, window_size, period):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "    seasonal_variance = np.nanvar(seasonal)\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (seasonal_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    normalized_seasonal_strength = seasonal_variance / (seasonal_variance + residual_variance) / mean_of_series # maybe remove (/s var + res var)\n",
    "    \n",
    "    return normalized_seasonal_strength\n",
    "\n",
    "df['train_seasonal_strength_all_30'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_seasonal_strength_all_7'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')\n",
    "df['train_seasonal_strength_1y_30'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_seasonal_strength_1y_7'] = df.apply(lambda row:calculate_seasonal_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/df_train_test_combs.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_train_test_combs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_sortino_ratio</th>\n",
       "      <th>test_sortino_ratio</th>\n",
       "      <th>train_slope_250</th>\n",
       "      <th>train_slope_100</th>\n",
       "      <th>train_slope_50</th>\n",
       "      <th>train_slope_20</th>\n",
       "      <th>train_seasonal_strength_all_30</th>\n",
       "      <th>train_seasonal_strength_all_7</th>\n",
       "      <th>train_seasonal_strength_1y_30</th>\n",
       "      <th>train_seasonal_strength_1y_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.098648</td>\n",
       "      <td>1.044396</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.997439</td>\n",
       "      <td>1.792124</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.005656</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.034292</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.927009</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>-0.011919</td>\n",
       "      <td>-0.024633</td>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.003238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.839687</td>\n",
       "      <td>1.130974</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>-0.010845</td>\n",
       "      <td>-0.002406</td>\n",
       "      <td>-0.092302</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.698057</td>\n",
       "      <td>1.374492</td>\n",
       "      <td>0.041044</td>\n",
       "      <td>-0.096869</td>\n",
       "      <td>0.086110</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_sortino_ratio  \\\n",
       "0              100.0             100.0  ...             2.098648   \n",
       "1              100.0             100.0  ...             1.997439   \n",
       "2              100.0             100.0  ...             1.927009   \n",
       "3              100.0             100.0  ...             1.839687   \n",
       "4              100.0             100.0  ...             1.698057   \n",
       "\n",
       "   test_sortino_ratio  train_slope_250  train_slope_100  train_slope_50  \\\n",
       "0            1.044396         0.002756        -0.002667        0.007090   \n",
       "1            1.792124         0.015681         0.000486       -0.005656   \n",
       "2            1.124046         0.024224        -0.011919       -0.024633   \n",
       "3            1.130974         0.009027        -0.010845       -0.002406   \n",
       "4            1.374492         0.041044        -0.096869        0.086110   \n",
       "\n",
       "   train_slope_20  train_seasonal_strength_all_30  \\\n",
       "0        0.015246                        0.002305   \n",
       "1        0.007873                        0.001074   \n",
       "2        0.070955                        0.001343   \n",
       "3       -0.092302                        0.000374   \n",
       "4        0.061747                        0.000220   \n",
       "\n",
       "   train_seasonal_strength_all_7  train_seasonal_strength_1y_30  \\\n",
       "0                       0.001010                       0.020462   \n",
       "1                       0.000352                       0.034292   \n",
       "2                       0.000313                       0.011229   \n",
       "3                       0.000404                       0.005329   \n",
       "4                       0.000175                       0.000893   \n",
       "\n",
       "   train_seasonal_strength_1y_7  \n",
       "0                      0.008107  \n",
       "1                      0.005128  \n",
       "2                      0.003238  \n",
       "3                      0.000967  \n",
       "4                      0.000275  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cyclical_strength(train_start, train_end, symbol, experiment_name, window_size, period, moving_avg_window=30):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the cyclical component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the cyclical strength.\n",
    "    period (int): The period of the cycle (e.g., 365 for annual cycles in daily data).\n",
    "    moving_avg_window (int): Window size for the moving average to approximate the cyclical component.\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized cyclical strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the residual component (contains cyclical + noise components)\n",
    "    residual = result.resid\n",
    "\n",
    "    # Apply moving average to the residuals to approximate the cyclical component\n",
    "    cyclical = residual.rolling(window=moving_avg_window, center=True).mean()\n",
    "\n",
    "    # Calculate variance of the cyclical component and the residuals, ignoring NaNs\n",
    "    cyclical_variance = np.nanvar(cyclical)\n",
    "    residual_variance = np.nanvar(residual - cyclical)  # Noise component variance\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    if mean_of_series == 0 or (cyclical_variance + residual_variance) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate normalized cyclical strength as a proportion of total variability\n",
    "    normalized_cyclical_strength = (cyclical_variance / (cyclical_variance + residual_variance)) / mean_of_series\n",
    "    \n",
    "    return normalized_cyclical_strength\n",
    "\n",
    "df['train_cyclical_strength_all_30'] = df.apply(lambda row:calculate_cyclical_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_cyclical_strength_all_7'] = df.apply(lambda row:calculate_cyclical_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    7,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')\n",
    "df['train_cyclical_strength_1y_30'] = df.apply(lambda row:calculate_cyclical_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_cyclical_strength_1y_7'] = df.apply(lambda row:calculate_cyclical_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    7,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_slope_50</th>\n",
       "      <th>train_slope_20</th>\n",
       "      <th>train_seasonal_strength_all_30</th>\n",
       "      <th>train_seasonal_strength_all_7</th>\n",
       "      <th>train_seasonal_strength_1y_30</th>\n",
       "      <th>train_seasonal_strength_1y_7</th>\n",
       "      <th>train_cyclical_strength_all_30</th>\n",
       "      <th>train_cyclical_strength_all_7</th>\n",
       "      <th>train_cyclical_strength_1y_30</th>\n",
       "      <th>train_cyclical_strength_1y_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.015246</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.012569</td>\n",
       "      <td>0.016994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005656</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.034292</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.011869</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.011182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024633</td>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.010083</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.003151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002406</td>\n",
       "      <td>-0.092302</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.002257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086110</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_slope_50  train_slope_20  \\\n",
       "0              100.0             100.0  ...        0.007090        0.015246   \n",
       "1              100.0             100.0  ...       -0.005656        0.007873   \n",
       "2              100.0             100.0  ...       -0.024633        0.070955   \n",
       "3              100.0             100.0  ...       -0.002406       -0.092302   \n",
       "4              100.0             100.0  ...        0.086110        0.061747   \n",
       "\n",
       "   train_seasonal_strength_all_30  train_seasonal_strength_all_7  \\\n",
       "0                        0.002305                       0.001010   \n",
       "1                        0.001074                       0.000352   \n",
       "2                        0.001343                       0.000313   \n",
       "3                        0.000374                       0.000404   \n",
       "4                        0.000220                       0.000175   \n",
       "\n",
       "   train_seasonal_strength_1y_30  train_seasonal_strength_1y_7  \\\n",
       "0                       0.020462                      0.008107   \n",
       "1                       0.034292                      0.005128   \n",
       "2                       0.011229                      0.003238   \n",
       "3                       0.005329                      0.000967   \n",
       "4                       0.000893                      0.000275   \n",
       "\n",
       "   train_cyclical_strength_all_30  train_cyclical_strength_all_7  \\\n",
       "0                        0.018082                       0.020867   \n",
       "1                        0.011869                       0.016063   \n",
       "2                        0.008606                       0.010083   \n",
       "3                        0.004765                       0.006648   \n",
       "4                        0.004904                       0.005346   \n",
       "\n",
       "   train_cyclical_strength_1y_30  train_cyclical_strength_1y_7  \n",
       "0                       0.012569                      0.016994  \n",
       "1                       0.006434                      0.011182  \n",
       "2                       0.002960                      0.003151  \n",
       "3                       0.001120                      0.002257  \n",
       "4                       0.002397                      0.002400  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_noise_strength(train_start, train_end, symbol, experiment_name, window_size, period):\n",
    "    \"\"\"\n",
    "    Calculate the normalized strength of the noise (residual) component of a time series over a given window.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window_size (int): Number of observations to use for calculating the noise strength.\n",
    "    period (int): The period of the seasonality (e.g., 365 for yearly seasonality on daily data).\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized noise strength value.\n",
    "    \"\"\"\n",
    "    \n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "    \n",
    "    # Perform time series decomposition (additive model)\n",
    "    result = seasonal_decompose(series_window, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Extract the seasonal and residual components\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Calculate variance of seasonal and residual components, ignoring NaNs\n",
    "    residual_variance = np.nanvar(residual)\n",
    "    \n",
    "    # Normalize by the mean of the original series to account for magnitude\n",
    "    mean_of_series = np.mean(series_window)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if mean_of_series == 0 or residual_variance == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate normalized noise strength\n",
    "    normalized_noise_strength = residual_variance / mean_of_series\n",
    "    \n",
    "    return normalized_noise_strength\n",
    "\n",
    "df['train_noise_strength_all_30'] = df.apply(lambda row:calculate_noise_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_noise_strength_all_7'] = df.apply(lambda row:calculate_noise_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')\n",
    "df['train_noise_strength_1y_30'] = df.apply(lambda row:calculate_noise_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    30), \n",
    "                                                axis = 'columns')\n",
    "df['train_noise_strength_1y_7'] = df.apply(lambda row:calculate_noise_strength(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250,\n",
    "                                                                                    7), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_seasonal_strength_1y_30</th>\n",
       "      <th>train_seasonal_strength_1y_7</th>\n",
       "      <th>train_cyclical_strength_all_30</th>\n",
       "      <th>train_cyclical_strength_all_7</th>\n",
       "      <th>train_cyclical_strength_1y_30</th>\n",
       "      <th>train_cyclical_strength_1y_7</th>\n",
       "      <th>train_noise_strength_all_30</th>\n",
       "      <th>train_noise_strength_all_7</th>\n",
       "      <th>train_noise_strength_1y_30</th>\n",
       "      <th>train_noise_strength_1y_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.012569</td>\n",
       "      <td>0.016994</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034292</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.011869</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>0.001444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.010083</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.001705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>0.004589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_seasonal_strength_1y_30  \\\n",
       "0              100.0             100.0  ...                       0.020462   \n",
       "1              100.0             100.0  ...                       0.034292   \n",
       "2              100.0             100.0  ...                       0.011229   \n",
       "3              100.0             100.0  ...                       0.005329   \n",
       "4              100.0             100.0  ...                       0.000893   \n",
       "\n",
       "   train_seasonal_strength_1y_7  train_cyclical_strength_all_30  \\\n",
       "0                      0.008107                        0.018082   \n",
       "1                      0.005128                        0.011869   \n",
       "2                      0.003238                        0.008606   \n",
       "3                      0.000967                        0.004765   \n",
       "4                      0.000275                        0.004904   \n",
       "\n",
       "   train_cyclical_strength_all_7  train_cyclical_strength_1y_30  \\\n",
       "0                       0.020867                       0.012569   \n",
       "1                       0.016063                       0.006434   \n",
       "2                       0.010083                       0.002960   \n",
       "3                       0.006648                       0.001120   \n",
       "4                       0.005346                       0.002397   \n",
       "\n",
       "   train_cyclical_strength_1y_7  train_noise_strength_all_30  \\\n",
       "0                      0.016994                     0.003266   \n",
       "1                      0.011182                     0.003913   \n",
       "2                      0.003151                     0.004609   \n",
       "3                      0.002257                     0.006438   \n",
       "4                      0.002400                     0.010295   \n",
       "\n",
       "   train_noise_strength_all_7  train_noise_strength_1y_30  \\\n",
       "0                    0.000746                    0.002173   \n",
       "1                    0.001099                    0.003438   \n",
       "2                    0.001295                    0.005662   \n",
       "3                    0.001950                    0.009533   \n",
       "4                    0.002484                    0.027465   \n",
       "\n",
       "   train_noise_strength_1y_7  \n",
       "0                   0.000487  \n",
       "1                   0.001444  \n",
       "2                   0.001705  \n",
       "3                   0.003646  \n",
       "4                   0.004589  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/df_train_test_combs.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_train_test_combs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_efficiency_ratio(train_start, train_end, symbol, experiment_name, window):\n",
    "    \"\"\"\n",
    "    Calculate the overall Efficiency Ratio (Fractal Efficiency) of a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    \n",
    "    Returns:\n",
    "    float: Efficiency Ratio for the entire series.\n",
    "    \"\"\"\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    if window_size:\n",
    "        series_window = series.tail(window_size)\n",
    "    else:\n",
    "        series_window = series\n",
    "        \n",
    "    # Calculate the Net Change over the entire series\n",
    "    net_change = abs(series_window.iloc[-1] - series_window.iloc[0])\n",
    "    \n",
    "    # Calculate the Total Change across all points in the series\n",
    "    total_change = series_window.diff().abs().sum()\n",
    "    \n",
    "    # Calculate the Efficiency Ratio\n",
    "    if total_change == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    \n",
    "    efficiency_ratio = net_change / total_change\n",
    "    return efficiency_ratio\n",
    "\n",
    "df['train_fract_efficiency_all'] = df.apply(lambda row:calculate_overall_efficiency_ratio(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    None), \n",
    "                                                axis = 'columns')\n",
    "df['train_fract_efficiency_250'] = df.apply(lambda row:calculate_overall_efficiency_ratio(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    250), \n",
    "                                                axis = 'columns')\n",
    "df['train_fract_efficiency_100'] = df.apply(lambda row:calculate_overall_efficiency_ratio(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    100), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent_of_max(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    s_max = series.max()\n",
    "    perc_of_max = series.tail(1)[0]/s_max\n",
    "    \n",
    "    return perc_of_max\n",
    "\n",
    "df['train_price_perc_from_max'] = df.apply(lambda row: calculate_percent_of_max(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name),\n",
    "                                           axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_drawdown(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    \n",
    "    s = series.pct_change()\n",
    "    max_drawdown = qs.stats.max_drawdown(s)\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]/len(series)\n",
    "    \n",
    "    return max_drawdown, max_drawdown_duration\n",
    "\n",
    "df[['train_max_drawdown', 'train_max_drawdown_duration']] = df.apply(lambda row: pd.Series(calculate_max_drawdown(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name)),\n",
    "                                           axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    \n",
    "    std = series.std()/series.mean()\n",
    "    return std\n",
    "\n",
    "df['train_price_std'] = df.apply(lambda row: calculate_std(row['train_start'], \n",
    "                                                            row['train_end'],\n",
    "                                                            row['symbol'], \n",
    "                                                            experiment_name),\n",
    "                                           axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_period_without_trades(symbol, experiment_name, train_test_comb):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    df_trades['exit_ts_shift'] = df_trades['Exit Timestamp'].shift(1)\n",
    "    period_length = (df_trades['Exit Timestamp'].max() - df_trades['Entry Timestamp'].min()).days\n",
    "    df_trades['days_between_trade'] = ((df_trades['Entry Timestamp'] - df_trades['exit_ts_shift']).dt.days)/period_length\n",
    "    min_days = df_trades['days_between_trade'].min()\n",
    "    max_days = df_trades['days_between_trade'].max()\n",
    "    mean_days = df_trades['days_between_trade'].mean()\n",
    "    std_days = df_trades['days_between_trade'].std()    \n",
    "    \n",
    "    return min_days, max_days, mean_days, std_days\n",
    "\n",
    "df[['train_period_wo_trades_min', 'train_period_wo_trades_max', \\\n",
    "    'train_period_wo_trades_mean', 'train_period_wo_trades_std']] = df.apply(lambda row: pd.Series(calculate_period_without_trades(\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    int(row['train_test_combination']))),\n",
    "                                                                axis = 'columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trades_per_period(symbol, experiment_name, train_test_comb):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    df_trades.set_index('Entry Timestamp', inplace = True)\n",
    "    df_g = df_trades.resample('M')['PnL'].count()\n",
    "    min_trades = df_g.min()\n",
    "    max_trades = df_g.max()\n",
    "    mean_trades = df_g.mean()\n",
    "    std_trades = df_g.std()\n",
    "    return min_trades, max_trades, mean_trades, std_trades\n",
    "\n",
    "df[['train_trades_per_period_min', 'train_trades_per_period_max', \\\n",
    "    'train_trades_per_period_mean', 'train_trades_per_period_std']] = df.apply(lambda row: pd.Series(calculate_trades_per_period(\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    int(row['train_test_combination']))),\n",
    "                                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trades_returns(symbol, experiment_name, train_test_comb):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    min_trades = df_trades['Return'].min()\n",
    "    max_trades = df_trades['Return'].max()\n",
    "    mean_trades = df_trades['Return'].mean()\n",
    "    std_trades = df_trades['Return'].std()\n",
    "    \n",
    "    return min_trades, max_trades, mean_trades, std_trades\n",
    "\n",
    "df[['train_trades_returns_min', 'train_trades_returns_max', \\\n",
    "    'train_trades_returns_mean', 'train_trades_returns_std']] = df.apply(lambda row: pd.Series(calculate_trades_returns(\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    int(row['train_test_combination']))),\n",
    "                                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_drawdown_period(train_start, train_end, symbol, experiment_name):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    \n",
    "    s = series.pct_change()\n",
    "    cummax = (1 + s).cumprod().cummax()\n",
    "    max_drawdown_duration = cummax.value_counts().head(1).iloc[0]\n",
    "    current_drawdown_duration = (cummax == cummax.tail(1)[0]).sum()\n",
    "    return current_drawdown_duration/max_drawdown_duration\n",
    "\n",
    "df['train_perc_drawd_per'] = df.apply(lambda row:calculate_perc_drawdown_period(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_returns_corr(train_start, train_end, symbol, experiment_name, train_test_comb):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                         index_col = 'Date')['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    df = pd.concat([series, (returns + 1).cumprod()], axis = 1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace = True)\n",
    "    corr = df.corr().iloc[0,1]\n",
    "    return corr\n",
    "\n",
    "df['train_price_returns_corr'] = df.apply(lambda row:calculate_price_returns_corr(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    row['train_test_combination']), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_returns_rolling_corr(train_start, train_end, symbol, experiment_name, train_test_comb, window):\n",
    "    series = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/df_price.csv\", \n",
    "                        index_col='Date', parse_dates=True)['Adj Close']\n",
    "    series = series[train_start:train_end]\n",
    "\n",
    "    # Load trade data\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "\n",
    "    # Prepare cumulative returns\n",
    "    returns = df_trades['Return']\n",
    "    returns.index = df_trades['Exit Timestamp']\n",
    "    returns.index = returns.index.floor('D')  # Align index to daily format\n",
    "    cumulative_returns = (returns + 1).cumprod()\n",
    "\n",
    "    # Combine and align series\n",
    "    df = pd.concat([series, cumulative_returns], axis=1)\n",
    "    df.iloc[0, 1] = 1\n",
    "    df.ffill(inplace=True)  # Handle missing data\n",
    "\n",
    "    # Calculate rolling correlation\n",
    "    rolling_corr = df['Adj Close'].rolling(window=window).corr(df['Return']).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    min_corr = rolling_corr.min()\n",
    "    max_corr = rolling_corr.max()\n",
    "    mean_corr = rolling_corr.mean()\n",
    "    std_corr = rolling_corr.std()\n",
    "\n",
    "    return min_corr, max_corr, mean_corr, std_corr\n",
    "\n",
    "df[['train_price_returns_corr_rol10_min', 'train_price_returns_corr_rol10_max', \\\n",
    "    'train_price_returns_corr_rol10_mean', 'train_price_returns_corr_rol10_std']] = df.apply(lambda row:pd.Series(calculate_price_returns_rolling_corr(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    row['train_test_combination'],\n",
    "                                                                                    10)), \n",
    "                                                axis = 'columns')\n",
    "df[['train_price_returns_corr_rol30_min', 'train_price_returns_corr_rol30_max', \\\n",
    "    'train_price_returns_corr_rol30_mean', 'train_price_returns_corr_rol30_std']] = df.apply(lambda row:pd.Series(calculate_price_returns_rolling_corr(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    row['train_test_combination'],\n",
    "                                                                                    30)), \n",
    "                                                axis = 'columns')\n",
    "df[['train_price_returns_corr_rol90_min', 'train_price_returns_corr_rol90_max', \\\n",
    "    'train_price_returns_corr_rol90_mean', 'train_price_returns_corr_rol90_std']] = df.apply(lambda row:pd.Series(calculate_price_returns_rolling_corr(row['train_start'], \n",
    "                                                                                    row['train_end'],\n",
    "                                                                                    row['symbol'], \n",
    "                                                                                    experiment_name,\n",
    "                                                                                    row['train_test_combination'],\n",
    "                                                                                    90)), \n",
    "                                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier_cnts(symbol, experiment_name, train_test_comb, threshold):\n",
    "    df_trades = pd.read_csv(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/backtest/train_{int(train_test_comb)}_trades.csv\")\n",
    "    df_trades[['Entry Timestamp', 'Exit Timestamp']] = df_trades[['Entry Timestamp', 'Exit Timestamp']].apply(pd.to_datetime)\n",
    "    \n",
    "    outliers_cnt = (df_trades['Return'] <= threshold).sum()\n",
    "    return outliers_cnt/len(df_trades)\n",
    "\n",
    "df['train_outliers_prc_neg01'] = df.apply(lambda row: calculate_outlier_cnts(\n",
    "                                                            row['symbol'], \n",
    "                                                            experiment_name,\n",
    "                                                            int(row['train_test_combination']),\n",
    "                                                            -0.1),\n",
    "                                        axis = 'columns')\n",
    "df['train_outliers_prc_neg015'] = df.apply(lambda row: calculate_outlier_cnts(\n",
    "                                                            row['symbol'], \n",
    "                                                            experiment_name,\n",
    "                                                            int(row['train_test_combination']),\n",
    "                                                            -0.15),\n",
    "                                        axis = 'columns')\n",
    "df['train_outliers_prc_neg02'] = df.apply(lambda row: calculate_outlier_cnts(\n",
    "                                                            row['symbol'], \n",
    "                                                            experiment_name,\n",
    "                                                            int(row['train_test_combination']),\n",
    "                                                            -0.2),\n",
    "                                        axis = 'columns')\n",
    "df['train_outliers_prc_neg03'] = df.apply(lambda row: calculate_outlier_cnts(\n",
    "                                                            row['symbol'], \n",
    "                                                            experiment_name,\n",
    "                                                            int(row['train_test_combination']),\n",
    "                                                            -0.3),\n",
    "                                        axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_report(symbol, experiment_name, train_test_comb):\n",
    "    train_stuff = pickle.load(open(f\"../outputs_hhhl_ml1/{experiment_name}/{symbol}/train_stuff.pickle\", 'rb'))\n",
    "    report = train_stuff['train_class_reports'][int(train_test_comb)]\n",
    "    \n",
    "    pattern = r'(\\b(?:False|True|accuracy|macro avg|weighted avg)\\b)\\s+([\\d.]*)\\s+([\\d.]+)\\s+([\\d.]+)'\n",
    "    matches = re.findall(pattern, report)\n",
    "\n",
    "    # Create dictionary to store metrics\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for match in matches:\n",
    "        label, precision, recall, f1_score = match\n",
    "        if label == 'accuracy':  # Only one value for accuracy\n",
    "            metrics_dict[label] = float(recall)\n",
    "        else:\n",
    "            if precision:  # Ignore empty precision values for rows without it\n",
    "                metrics_dict[f\"{label}_precision\"] = float(precision)\n",
    "            metrics_dict[f\"{label}_recall\"] = float(recall)\n",
    "            metrics_dict[f\"{label}_f1_score\"] = float(f1_score)\n",
    "    prefix = 'train_'\n",
    "    return_dict = {f\"{prefix}{key.lower()}\": value for key, value in metrics_dict.items()}\n",
    "    return return_dict\n",
    "\n",
    "df_model_metrics = pd.DataFrame(df.apply(lambda row: pd.Series(get_class_report(row['symbol'],\n",
    "                                                         experiment_name,\n",
    "                                                         row['train_test_combination'])),\n",
    "                            axis = 'columns'))\n",
    "df = pd.concat([df, df_model_metrics], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_true_precision</th>\n",
       "      <th>train_true_recall</th>\n",
       "      <th>train_true_f1_score</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_macro avg_precision</th>\n",
       "      <th>train_macro avg_recall</th>\n",
       "      <th>train_macro avg_f1_score</th>\n",
       "      <th>train_weighted avg_precision</th>\n",
       "      <th>train_weighted avg_recall</th>\n",
       "      <th>train_weighted avg_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_true_precision  \\\n",
       "0              100.0             100.0  ...                  0.94   \n",
       "1              100.0             100.0  ...                  0.92   \n",
       "2              100.0             100.0  ...                  0.92   \n",
       "3              100.0             100.0  ...                  0.91   \n",
       "4              100.0             100.0  ...                  0.90   \n",
       "\n",
       "   train_true_recall  train_true_f1_score  train_accuracy  \\\n",
       "0               0.98                 0.96            0.97   \n",
       "1               0.97                 0.94            0.96   \n",
       "2               0.96                 0.94            0.95   \n",
       "3               0.94                 0.93            0.94   \n",
       "4               0.93                 0.92            0.93   \n",
       "\n",
       "   train_macro avg_precision  train_macro avg_recall  \\\n",
       "0                       0.96                    0.97   \n",
       "1                       0.95                    0.96   \n",
       "2                       0.95                    0.95   \n",
       "3                       0.94                    0.94   \n",
       "4                       0.93                    0.93   \n",
       "\n",
       "   train_macro avg_f1_score  train_weighted avg_precision  \\\n",
       "0                      0.97                          0.97   \n",
       "1                      0.95                          0.96   \n",
       "2                      0.95                          0.95   \n",
       "3                      0.94                          0.94   \n",
       "4                      0.93                          0.93   \n",
       "\n",
       "   train_weighted avg_recall  train_weighted avg_f1_score  \n",
       "0                       0.97                         0.97  \n",
       "1                       0.96                         0.96  \n",
       "2                       0.95                         0.95  \n",
       "3                       0.94                         0.94  \n",
       "4                       0.93                         0.93  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symbol',\n",
       " 'train_test_combination',\n",
       " 'train_start',\n",
       " 'test_start',\n",
       " 'train_end',\n",
       " 'test_end',\n",
       " 'train_period',\n",
       " 'test_period',\n",
       " 'train_start_value',\n",
       " 'test_start_value',\n",
       " 'train_end_value',\n",
       " 'test_end_value',\n",
       " 'train_total_return_[%]',\n",
       " 'test_total_return_[%]',\n",
       " 'train_benchmark_return_[%]',\n",
       " 'test_benchmark_return_[%]',\n",
       " 'train_max_gross_exposure_[%]',\n",
       " 'test_max_gross_exposure_[%]',\n",
       " 'train_total_fees_paid',\n",
       " 'test_total_fees_paid',\n",
       " 'train_max_drawdown_[%]',\n",
       " 'test_max_drawdown_[%]',\n",
       " 'train_max_drawdown_duration',\n",
       " 'test_max_drawdown_duration',\n",
       " 'train_total_trades',\n",
       " 'test_total_trades',\n",
       " 'train_total_closed_trades',\n",
       " 'test_total_closed_trades',\n",
       " 'train_total_open_trades',\n",
       " 'test_total_open_trades',\n",
       " 'train_open_trade_pnl',\n",
       " 'test_open_trade_pnl',\n",
       " 'train_win_rate_[%]',\n",
       " 'test_win_rate_[%]',\n",
       " 'train_best_trade_[%]',\n",
       " 'test_best_trade_[%]',\n",
       " 'train_worst_trade_[%]',\n",
       " 'test_worst_trade_[%]',\n",
       " 'train_avg_winning_trade_[%]',\n",
       " 'test_avg_winning_trade_[%]',\n",
       " 'train_avg_losing_trade_[%]',\n",
       " 'test_avg_losing_trade_[%]',\n",
       " 'train_avg_winning_trade_duration',\n",
       " 'test_avg_winning_trade_duration',\n",
       " 'train_avg_losing_trade_duration',\n",
       " 'test_avg_losing_trade_duration',\n",
       " 'train_profit_factor',\n",
       " 'test_profit_factor',\n",
       " 'train_expectancy',\n",
       " 'test_expectancy',\n",
       " 'train_sharpe_ratio',\n",
       " 'test_sharpe_ratio',\n",
       " 'train_calmar_ratio',\n",
       " 'test_calmar_ratio',\n",
       " 'train_omega_ratio',\n",
       " 'test_omega_ratio',\n",
       " 'train_sortino_ratio',\n",
       " 'test_sortino_ratio',\n",
       " 'train_slope_250',\n",
       " 'train_slope_100',\n",
       " 'train_slope_50',\n",
       " 'train_slope_20',\n",
       " 'train_seasonal_strength_all_30',\n",
       " 'train_seasonal_strength_all_7',\n",
       " 'train_seasonal_strength_1y_30',\n",
       " 'train_seasonal_strength_1y_7',\n",
       " 'train_cyclical_strength_all_30',\n",
       " 'train_cyclical_strength_all_7',\n",
       " 'train_cyclical_strength_1y_30',\n",
       " 'train_cyclical_strength_1y_7',\n",
       " 'train_noise_strength_all_30',\n",
       " 'train_noise_strength_all_7',\n",
       " 'train_noise_strength_1y_30',\n",
       " 'train_noise_strength_1y_7',\n",
       " 'train_fract_efficiency_all',\n",
       " 'train_fract_efficiency_250',\n",
       " 'train_fract_efficiency_100',\n",
       " 'train_price_perc_from_max',\n",
       " 'train_max_drawdown',\n",
       " 'train_price_std',\n",
       " 'train_period_wo_trades_min',\n",
       " 'train_period_wo_trades_max',\n",
       " 'train_period_wo_trades_mean',\n",
       " 'train_period_wo_trades_std',\n",
       " 'train_trades_per_period_min',\n",
       " 'train_trades_per_period_max',\n",
       " 'train_trades_per_period_mean',\n",
       " 'train_trades_per_period_std',\n",
       " 'train_trades_returns_min',\n",
       " 'train_trades_returns_max',\n",
       " 'train_trades_returns_mean',\n",
       " 'train_trades_returns_std',\n",
       " 'train_perc_drawd_per',\n",
       " 'train_price_returns_corr',\n",
       " 'train_price_returns_corr_rol10_min',\n",
       " 'train_price_returns_corr_rol10_max',\n",
       " 'train_price_returns_corr_rol10_mean',\n",
       " 'train_price_returns_corr_rol10_std',\n",
       " 'train_price_returns_corr_rol30_min',\n",
       " 'train_price_returns_corr_rol30_max',\n",
       " 'train_price_returns_corr_rol30_mean',\n",
       " 'train_price_returns_corr_rol30_std',\n",
       " 'train_price_returns_corr_rol90_min',\n",
       " 'train_price_returns_corr_rol90_max',\n",
       " 'train_price_returns_corr_rol90_mean',\n",
       " 'train_price_returns_corr_rol90_std',\n",
       " 'train_outliers_prc_neg01',\n",
       " 'train_outliers_prc_neg015',\n",
       " 'train_outliers_prc_neg02',\n",
       " 'train_outliers_prc_neg03',\n",
       " 'train_false_precision',\n",
       " 'train_false_recall',\n",
       " 'train_false_f1_score',\n",
       " 'train_true_precision',\n",
       " 'train_true_recall',\n",
       " 'train_true_f1_score',\n",
       " 'train_accuracy',\n",
       " 'train_macro avg_precision',\n",
       " 'train_macro avg_recall',\n",
       " 'train_macro avg_f1_score',\n",
       " 'train_weighted avg_precision',\n",
       " 'train_weighted avg_recall',\n",
       " 'train_weighted avg_f1_score']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('../data/df_train_test_combs.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_train_test_combs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>train_test_combination</th>\n",
       "      <th>train_start</th>\n",
       "      <th>test_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>train_start_value</th>\n",
       "      <th>test_start_value</th>\n",
       "      <th>...</th>\n",
       "      <th>train_true_precision</th>\n",
       "      <th>train_true_recall</th>\n",
       "      <th>train_true_f1_score</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_macro avg_precision</th>\n",
       "      <th>train_macro avg_recall</th>\n",
       "      <th>train_macro avg_f1_score</th>\n",
       "      <th>train_weighted avg_precision</th>\n",
       "      <th>train_weighted avg_recall</th>\n",
       "      <th>train_weighted avg_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TJX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2006-08-07 00:00:00</td>\n",
       "      <td>2006-08-04 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TJX</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2010-01-28 00:00:00</td>\n",
       "      <td>2010-01-27 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TJX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2012-11-30 00:00:00</td>\n",
       "      <td>2012-11-29 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TJX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>2016-01-25 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>4036.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>2019-02-13 00:00:00</td>\n",
       "      <td>2019-02-12 00:00:00</td>\n",
       "      <td>2021-12-27 00:00:00</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  train_test_combination          train_start           test_start  \\\n",
       "0    TJX                     0.0  2000-01-07 00:00:00  2006-08-07 00:00:00   \n",
       "1    TJX                     1.0  2000-01-07 00:00:00  2010-01-28 00:00:00   \n",
       "2    TJX                     2.0  2000-01-07 00:00:00  2012-11-30 00:00:00   \n",
       "3    TJX                     3.0  2000-01-07 00:00:00  2016-01-26 00:00:00   \n",
       "4    TJX                     4.0  2000-01-07 00:00:00  2019-02-13 00:00:00   \n",
       "\n",
       "             train_end             test_end  train_period  test_period  \\\n",
       "0  2006-08-04 00:00:00  2010-01-27 00:00:00        1653.0        875.0   \n",
       "1  2010-01-27 00:00:00  2012-11-29 00:00:00        2528.0        716.0   \n",
       "2  2012-11-29 00:00:00  2016-01-25 00:00:00        3244.0        792.0   \n",
       "3  2016-01-25 00:00:00  2019-02-12 00:00:00        4036.0        768.0   \n",
       "4  2019-02-12 00:00:00  2021-12-27 00:00:00        4804.0        724.0   \n",
       "\n",
       "   train_start_value  test_start_value  ...  train_true_precision  \\\n",
       "0              100.0             100.0  ...                  0.94   \n",
       "1              100.0             100.0  ...                  0.92   \n",
       "2              100.0             100.0  ...                  0.92   \n",
       "3              100.0             100.0  ...                  0.91   \n",
       "4              100.0             100.0  ...                  0.90   \n",
       "\n",
       "   train_true_recall  train_true_f1_score  train_accuracy  \\\n",
       "0               0.98                 0.96            0.97   \n",
       "1               0.97                 0.94            0.96   \n",
       "2               0.96                 0.94            0.95   \n",
       "3               0.94                 0.93            0.94   \n",
       "4               0.93                 0.92            0.93   \n",
       "\n",
       "   train_macro avg_precision  train_macro avg_recall  \\\n",
       "0                       0.96                    0.97   \n",
       "1                       0.95                    0.96   \n",
       "2                       0.95                    0.95   \n",
       "3                       0.94                    0.94   \n",
       "4                       0.93                    0.93   \n",
       "\n",
       "   train_macro avg_f1_score  train_weighted avg_precision  \\\n",
       "0                      0.97                          0.97   \n",
       "1                      0.95                          0.96   \n",
       "2                      0.95                          0.95   \n",
       "3                      0.94                          0.94   \n",
       "4                      0.93                          0.93   \n",
       "\n",
       "   train_weighted avg_recall  train_weighted avg_f1_score  \n",
       "0                       0.97                         0.97  \n",
       "1                       0.96                         0.96  \n",
       "2                       0.95                         0.95  \n",
       "3                       0.94                         0.94  \n",
       "4                       0.93                         0.93  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_max_drawdown_[%]</th>\n",
       "      <th>train_max_drawdown_duration</th>\n",
       "      <th>train_win_rate_[%]</th>\n",
       "      <th>train_best_trade_[%]</th>\n",
       "      <th>train_worst_trade_[%]</th>\n",
       "      <th>train_avg_winning_trade_[%]</th>\n",
       "      <th>train_avg_losing_trade_[%]</th>\n",
       "      <th>train_avg_winning_trade_duration</th>\n",
       "      <th>train_avg_losing_trade_duration</th>\n",
       "      <th>train_profit_factor</th>\n",
       "      <th>...</th>\n",
       "      <th>train_true_precision</th>\n",
       "      <th>train_true_recall</th>\n",
       "      <th>train_true_f1_score</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_macro avg_precision</th>\n",
       "      <th>train_macro avg_recall</th>\n",
       "      <th>train_macro avg_f1_score</th>\n",
       "      <th>train_weighted avg_precision</th>\n",
       "      <th>train_weighted avg_recall</th>\n",
       "      <th>train_weighted avg_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.842808</td>\n",
       "      <td>0.323245</td>\n",
       "      <td>55.319149</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>-8.695652</td>\n",
       "      <td>4.503891</td>\n",
       "      <td>-1.626228</td>\n",
       "      <td>7.076923</td>\n",
       "      <td>2.803279</td>\n",
       "      <td>2.872888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.095343</td>\n",
       "      <td>0.212505</td>\n",
       "      <td>57.711443</td>\n",
       "      <td>34.033639</td>\n",
       "      <td>-7.347840</td>\n",
       "      <td>4.492853</td>\n",
       "      <td>-1.728899</td>\n",
       "      <td>7.146552</td>\n",
       "      <td>2.987952</td>\n",
       "      <td>2.461184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.871764</td>\n",
       "      <td>0.165587</td>\n",
       "      <td>69.709544</td>\n",
       "      <td>34.033639</td>\n",
       "      <td>-8.695652</td>\n",
       "      <td>3.846482</td>\n",
       "      <td>-1.791416</td>\n",
       "      <td>7.613095</td>\n",
       "      <td>3.408451</td>\n",
       "      <td>5.409532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.956289</td>\n",
       "      <td>0.133086</td>\n",
       "      <td>68.896321</td>\n",
       "      <td>47.899160</td>\n",
       "      <td>-8.808931</td>\n",
       "      <td>3.829873</td>\n",
       "      <td>-1.669257</td>\n",
       "      <td>7.349515</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>4.509006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.921668</td>\n",
       "      <td>0.111805</td>\n",
       "      <td>67.673716</td>\n",
       "      <td>47.899160</td>\n",
       "      <td>-9.358977</td>\n",
       "      <td>3.718765</td>\n",
       "      <td>-1.572589</td>\n",
       "      <td>8.245536</td>\n",
       "      <td>3.431373</td>\n",
       "      <td>5.106710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_max_drawdown_[%]  train_max_drawdown_duration  train_win_rate_[%]  \\\n",
       "0               32.842808                     0.323245           55.319149   \n",
       "1               32.095343                     0.212505           57.711443   \n",
       "2               31.871764                     0.165587           69.709544   \n",
       "3               31.956289                     0.133086           68.896321   \n",
       "4               32.921668                     0.111805           67.673716   \n",
       "\n",
       "   train_best_trade_[%]  train_worst_trade_[%]  train_avg_winning_trade_[%]  \\\n",
       "0             29.411765              -8.695652                     4.503891   \n",
       "1             34.033639              -7.347840                     4.492853   \n",
       "2             34.033639              -8.695652                     3.846482   \n",
       "3             47.899160              -8.808931                     3.829873   \n",
       "4             47.899160              -9.358977                     3.718765   \n",
       "\n",
       "   train_avg_losing_trade_[%]  train_avg_winning_trade_duration  \\\n",
       "0                   -1.626228                          7.076923   \n",
       "1                   -1.728899                          7.146552   \n",
       "2                   -1.791416                          7.613095   \n",
       "3                   -1.669257                          7.349515   \n",
       "4                   -1.572589                          8.245536   \n",
       "\n",
       "   train_avg_losing_trade_duration  train_profit_factor  ...  \\\n",
       "0                         2.803279             2.872888  ...   \n",
       "1                         2.987952             2.461184  ...   \n",
       "2                         3.408451             5.409532  ...   \n",
       "3                         3.200000             4.509006  ...   \n",
       "4                         3.431373             5.106710  ...   \n",
       "\n",
       "   train_true_precision  train_true_recall  train_true_f1_score  \\\n",
       "0                  0.94               0.98                 0.96   \n",
       "1                  0.92               0.97                 0.94   \n",
       "2                  0.92               0.96                 0.94   \n",
       "3                  0.91               0.94                 0.93   \n",
       "4                  0.90               0.93                 0.92   \n",
       "\n",
       "   train_accuracy  train_macro avg_precision  train_macro avg_recall  \\\n",
       "0            0.97                       0.96                    0.97   \n",
       "1            0.96                       0.95                    0.96   \n",
       "2            0.95                       0.95                    0.95   \n",
       "3            0.94                       0.94                    0.94   \n",
       "4            0.93                       0.93                    0.93   \n",
       "\n",
       "   train_macro avg_f1_score  train_weighted avg_precision  \\\n",
       "0                      0.97                          0.97   \n",
       "1                      0.95                          0.96   \n",
       "2                      0.95                          0.95   \n",
       "3                      0.94                          0.94   \n",
       "4                      0.93                          0.93   \n",
       "\n",
       "   train_weighted avg_recall  train_weighted avg_f1_score  \n",
       "0                       0.97                         0.97  \n",
       "1                       0.96                         0.96  \n",
       "2                       0.95                         0.95  \n",
       "3                       0.94                         0.94  \n",
       "4                       0.93                         0.93  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df['train_test_combination'].isin([0,1,2,3,4,5])]\n",
    "train_cols_to_drop = ['symbol', 'train_test_combination', 'train_start', 'train_end', \n",
    "                'train_period', 'train_start_value',  'train_end_value',\n",
    "               'train_total_return_[%]', 'train_benchmark_return_[%]', 'train_max_gross_exposure_[%]',\n",
    "               'train_total_fees_paid', 'train_total_trades', 'train_total_closed_trades',\n",
    "               'train_total_open_trades', 'train_open_trade_pnl', 'train_expectancy']\n",
    "\n",
    "question = ['train_profit_factor', 'train_expectancy']\n",
    "\n",
    "target = 'test_sharpe_ratio'\n",
    "df_fin = df.drop(columns = train_cols_to_drop)\n",
    "test_cols_to_drop = [col for col in df_fin.columns if ('test' in col) and (col != target)]\n",
    "df_fin = df_fin.drop(columns = test_cols_to_drop)\n",
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1052, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best Model Parameters: {'model': RandomForestRegressor(), 'model__max_depth': None, 'model__n_estimators': 100}\n",
      "\n",
      "Test MSE: 0.2910421224033346\n",
      "Test RÂ²: 0.45609336449805593\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate example dataset\n",
    "np.random.seed(55)\n",
    "df_fin = df_fin[~df_fin.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "x = df_fin.loc[:, [col for col in df_fin.columns if col != 'test_sharpe_ratio']]\n",
    "y = df_fin['test_sharpe_ratio']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=55)\n",
    "\n",
    "# Define models and hyperparameter grids\n",
    "param_grid = [\n",
    "    {\n",
    "        'model': [ElasticNet()],\n",
    "        'model__alpha': [0.1, 1.0, 10.0],\n",
    "        'model__l1_ratio': [0.1, 0.5, 0.9]\n",
    "    },\n",
    "    {\n",
    "        'model': [RandomForestRegressor()],\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "    },\n",
    "    {\n",
    "        'model': [SVR()],\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__gamma': ['scale', 'auto']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Normalizes features\n",
    "    ('model', ElasticNet())        # Placeholder model\n",
    "])\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and its performance\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Model Parameters:\", best_params)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nTest MSE:\", mse)\n",
    "print(\"Test RÂ²:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test MSE: 0.34719580032255015\n",
      "Test RÂ²: 0.1598239915729237\n"
     ]
    }
   ],
   "source": [
    "# experiment results where model training is on 0-4 train-test-combs, and eval is on 5\n",
    "# # Generate example dataset\n",
    "# np.random.seed(55)\n",
    "# df_fin = df_fin[~df_fin.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "# x = df_fin.loc[:, [col for col in df_fin.columns if col != 'test_sharpe_ratio']]\n",
    "# y = df_fin['test_sharpe_ratio']\n",
    "# y_test = y\n",
    "\n",
    "# # Evaluate on test data - the 5 train-test-comb\n",
    "# y_pred = best_model.predict(x)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(\"\\nTest MSE:\", mse)\n",
    "# print(\"Test RÂ²:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_sharpe_ratio</td>\n",
       "      <td>0.299103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>train_period_wo_trades_min</td>\n",
       "      <td>0.047764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>train_trades_returns_mean</td>\n",
       "      <td>0.035498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_avg_winning_trade_[%]</td>\n",
       "      <td>0.031288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_calmar_ratio</td>\n",
       "      <td>0.025191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>train_weighted avg_f1_score</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>train_weighted avg_recall</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>train_noise_strength_all_7</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>train_trades_per_period_min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature  Importance\n",
       "10           train_sharpe_ratio    0.299103\n",
       "36   train_period_wo_trades_min    0.047764\n",
       "46    train_trades_returns_mean    0.035498\n",
       "5   train_avg_winning_trade_[%]    0.031288\n",
       "11           train_calmar_ratio    0.025191\n",
       "..                          ...         ...\n",
       "78  train_weighted avg_f1_score    0.000670\n",
       "77    train_weighted avg_recall    0.000643\n",
       "72               train_accuracy    0.000555\n",
       "27   train_noise_strength_all_7    0.000303\n",
       "40  train_trades_per_period_min    0.000000\n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = best_model[1].feature_importances_\n",
    "\n",
    "# Assuming X_train is a pandas DataFrame with column names (features)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Feature names\n",
    "    'Importance': feature_importances  # Feature importances\n",
    "})\n",
    "\n",
    "# Sort the features by their importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Ensure y_test and y_pred are numpy arrays\n",
    "y_test_array = y_test.to_numpy().reshape(-1, 1)  # Convert to 2D array\n",
    "y_pred_array = y_pred.reshape(-1, 1)  # Ensure y_pred is 2D as well\n",
    "\n",
    "# Fit a regression line to the scatter data\n",
    "reg_line = LinearRegression()\n",
    "reg_line.fit(y_test_array, y_pred_array)  # Fit the regression line\n",
    "line_x = np.linspace(min(y_test_array), max(y_test_array), 100).reshape(-1, 1)  # Generate X values for line\n",
    "line_y = reg_line.predict(line_x)  # Predict Y values for the line\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, label=\"Predicted vs Actual\")\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(line_x, line_y, color=\"red\", label=\"Regression Line\", linestyle=\"--\")\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title(\"Scatter Plot: Actual vs Predicted with Regression Line\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select symbols for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47200917, 1.44499481, 1.04109947, 1.81943009, 1.06274596,\n",
       "       1.16703272, 1.2330774 , 1.38417791, 2.17133292, 2.23887836,\n",
       "       2.17938561, 1.08499439, 1.22785183, 1.97685418, 1.62298206,\n",
       "       1.80989834, 1.62296308, 1.67302124, 1.26940479, 1.74445272,\n",
       "       1.44314126, 1.61058542, 0.97664915, 1.13675233, 1.36288041,\n",
       "       1.5462547 , 2.28620131, 1.42516843, 1.9522561 , 1.2296546 ,\n",
       "       1.0278519 , 0.94257775, 1.06696533, 1.87880933, 1.93787148,\n",
       "       1.41252313, 1.47005478, 2.93919749, 1.79841071, 1.40956788,\n",
       "       1.54222494, 1.91789946, 1.50528947, 1.25160333, 1.61806692,\n",
       "       1.00838703, 1.09517358, 0.88924664, 1.34089171, 2.05432977,\n",
       "       1.17236623, 0.93404697, 1.42636625, 1.04829402, 1.68478835,\n",
       "       1.42361279, 1.27998305, 2.3475731 , 1.28250345, 0.99907525,\n",
       "       1.17356285, 1.29978474, 1.43539367, 1.41574384, 2.76911114,\n",
       "       1.34805764, 1.63133645, 1.81176295, 1.11293174, 1.251573  ,\n",
       "       1.7201539 , 3.12712617, 1.33266645, 1.12402684, 1.02347412,\n",
       "       1.21605137, 1.11881769, 0.97653465, 1.38087957, 0.99242076,\n",
       "       2.47655354, 2.18784015, 1.83934053, 1.26863499, 1.69495663,\n",
       "       1.20377863, 1.03309486, 1.1571144 , 1.06281956, 1.49384284,\n",
       "       1.5638766 , 1.22279965, 0.39718228, 2.09278529, 1.47950992,\n",
       "       1.4393091 , 1.35399975, 1.09153721, 1.10982126, 1.15372062,\n",
       "       1.15028908, 0.98838488, 1.44410248, 1.32967045, 1.16976217,\n",
       "       1.24492717, 1.30336762, 1.3285949 , 1.67508678, 1.97681147,\n",
       "       1.61467462, 1.17502435, 1.05360343, 1.489163  , 1.2830221 ,\n",
       "       1.25533578, 1.32502323, 1.23460392, 1.55178972, 1.2722135 ,\n",
       "       1.79545508, 1.29698122, 2.35175452, 1.53355066, 1.16858812,\n",
       "       1.50563786, 1.32858754, 1.21254608, 1.4508952 , 1.31221544,\n",
       "       1.64720278, 2.42776557, 1.76184559, 1.40538292, 2.0373261 ,\n",
       "       1.74832828, 1.57120583, 1.70432564, 1.53615778, 2.73159002,\n",
       "       2.09274297, 1.02852579, 1.19188778, 1.17012654, 1.58705787,\n",
       "       1.32461096, 1.1862168 , 1.98023122, 1.32421944, 1.26256387,\n",
       "       0.58433522, 1.16886085, 1.31898492, 1.92125486, 1.98482966,\n",
       "       1.213248  , 2.04557998, 2.81593413, 0.99809947, 1.4079857 ,\n",
       "       1.93009129, 1.67287121, 1.39994682, 1.16611137, 1.30765531,\n",
       "       2.07421185, 1.22328524, 1.29895882, 1.04376458, 1.38775637,\n",
       "       2.09406928, 2.20612121, 1.43719009, 1.17554842, 1.45867399,\n",
       "       1.32410695, 1.08226002, 1.42250037, 1.20975505, 1.46076287,\n",
       "       1.42309568, 1.82556688, 0.89158006, 1.13882062, 1.42406843,\n",
       "       1.59893204, 1.567485  , 1.08520716, 1.04050997, 1.49429245,\n",
       "       2.20927117, 1.38053334, 1.41575237, 1.64817152, 1.16134049,\n",
       "       1.69166159, 1.84987368, 1.60410883, 1.19990103, 0.98987314,\n",
       "       1.01090684, 1.06223663, 2.04280224, 1.78208006, 1.37716097,\n",
       "       1.37898697, 1.01228975, 1.41675738, 1.39548109, 1.09765962,\n",
       "       1.11408731])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_max_drawdown_[%]</th>\n",
       "      <th>train_max_drawdown_duration</th>\n",
       "      <th>train_win_rate_[%]</th>\n",
       "      <th>train_best_trade_[%]</th>\n",
       "      <th>train_worst_trade_[%]</th>\n",
       "      <th>train_avg_winning_trade_[%]</th>\n",
       "      <th>train_avg_losing_trade_[%]</th>\n",
       "      <th>train_avg_winning_trade_duration</th>\n",
       "      <th>train_avg_losing_trade_duration</th>\n",
       "      <th>train_profit_factor</th>\n",
       "      <th>...</th>\n",
       "      <th>train_true_precision</th>\n",
       "      <th>train_true_recall</th>\n",
       "      <th>train_true_f1_score</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_macro avg_precision</th>\n",
       "      <th>train_macro avg_recall</th>\n",
       "      <th>train_macro avg_f1_score</th>\n",
       "      <th>train_weighted avg_precision</th>\n",
       "      <th>train_weighted avg_recall</th>\n",
       "      <th>train_weighted avg_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>42.860599</td>\n",
       "      <td>0.635988</td>\n",
       "      <td>66.935484</td>\n",
       "      <td>21.367965</td>\n",
       "      <td>-4.122223</td>\n",
       "      <td>5.306573</td>\n",
       "      <td>-1.560548</td>\n",
       "      <td>9.578313</td>\n",
       "      <td>2.731707</td>\n",
       "      <td>8.745014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>62.912072</td>\n",
       "      <td>0.981417</td>\n",
       "      <td>65.683646</td>\n",
       "      <td>25.408502</td>\n",
       "      <td>-42.915115</td>\n",
       "      <td>4.150974</td>\n",
       "      <td>-2.313046</td>\n",
       "      <td>7.632653</td>\n",
       "      <td>3.171875</td>\n",
       "      <td>4.160977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>53.339278</td>\n",
       "      <td>0.418415</td>\n",
       "      <td>63.519313</td>\n",
       "      <td>28.678680</td>\n",
       "      <td>-35.912733</td>\n",
       "      <td>3.300890</td>\n",
       "      <td>-2.024583</td>\n",
       "      <td>8.121622</td>\n",
       "      <td>3.448718</td>\n",
       "      <td>4.014227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>54.748999</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>55.238095</td>\n",
       "      <td>18.081659</td>\n",
       "      <td>-6.442264</td>\n",
       "      <td>3.114612</td>\n",
       "      <td>-1.397419</td>\n",
       "      <td>8.948276</td>\n",
       "      <td>2.478261</td>\n",
       "      <td>2.793046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>38.266356</td>\n",
       "      <td>0.217986</td>\n",
       "      <td>72.647059</td>\n",
       "      <td>10.367452</td>\n",
       "      <td>-4.636569</td>\n",
       "      <td>2.315507</td>\n",
       "      <td>-0.907811</td>\n",
       "      <td>9.449393</td>\n",
       "      <td>3.347826</td>\n",
       "      <td>5.855560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>27.935003</td>\n",
       "      <td>0.331674</td>\n",
       "      <td>66.153846</td>\n",
       "      <td>32.071262</td>\n",
       "      <td>-7.778710</td>\n",
       "      <td>2.686399</td>\n",
       "      <td>-1.433349</td>\n",
       "      <td>7.662791</td>\n",
       "      <td>3.068182</td>\n",
       "      <td>3.657179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>39.257492</td>\n",
       "      <td>0.335418</td>\n",
       "      <td>69.892473</td>\n",
       "      <td>41.351152</td>\n",
       "      <td>-19.755170</td>\n",
       "      <td>8.476595</td>\n",
       "      <td>-4.490451</td>\n",
       "      <td>7.623077</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>2.191529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>24.477366</td>\n",
       "      <td>0.727869</td>\n",
       "      <td>49.382716</td>\n",
       "      <td>27.714299</td>\n",
       "      <td>-7.801421</td>\n",
       "      <td>6.478491</td>\n",
       "      <td>-1.739112</td>\n",
       "      <td>8.725000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>4.033304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>40.372223</td>\n",
       "      <td>0.515045</td>\n",
       "      <td>61.603376</td>\n",
       "      <td>15.386495</td>\n",
       "      <td>-5.707196</td>\n",
       "      <td>3.003991</td>\n",
       "      <td>-1.192451</td>\n",
       "      <td>7.904110</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>4.821732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>62.971219</td>\n",
       "      <td>0.173998</td>\n",
       "      <td>67.286245</td>\n",
       "      <td>18.222640</td>\n",
       "      <td>-8.184396</td>\n",
       "      <td>3.609095</td>\n",
       "      <td>-1.243985</td>\n",
       "      <td>10.104972</td>\n",
       "      <td>3.602273</td>\n",
       "      <td>5.267913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_max_drawdown_[%]  train_max_drawdown_duration  train_win_rate_[%]  \\\n",
       "751               42.860599                     0.635988           66.935484   \n",
       "598               62.912072                     0.981417           65.683646   \n",
       "162               53.339278                     0.418415           63.519313   \n",
       "91                54.748999                     0.221374           55.238095   \n",
       "88                38.266356                     0.217986           72.647059   \n",
       "..                      ...                          ...                 ...   \n",
       "710               27.935003                     0.331674           66.153846   \n",
       "315               39.257492                     0.335418           69.892473   \n",
       "48                24.477366                     0.727869           49.382716   \n",
       "274               40.372223                     0.515045           61.603376   \n",
       "30                62.971219                     0.173998           67.286245   \n",
       "\n",
       "     train_best_trade_[%]  train_worst_trade_[%]  train_avg_winning_trade_[%]  \\\n",
       "751             21.367965              -4.122223                     5.306573   \n",
       "598             25.408502             -42.915115                     4.150974   \n",
       "162             28.678680             -35.912733                     3.300890   \n",
       "91              18.081659              -6.442264                     3.114612   \n",
       "88              10.367452              -4.636569                     2.315507   \n",
       "..                    ...                    ...                          ...   \n",
       "710             32.071262              -7.778710                     2.686399   \n",
       "315             41.351152             -19.755170                     8.476595   \n",
       "48              27.714299              -7.801421                     6.478491   \n",
       "274             15.386495              -5.707196                     3.003991   \n",
       "30              18.222640              -8.184396                     3.609095   \n",
       "\n",
       "     train_avg_losing_trade_[%]  train_avg_winning_trade_duration  \\\n",
       "751                   -1.560548                          9.578313   \n",
       "598                   -2.313046                          7.632653   \n",
       "162                   -2.024583                          8.121622   \n",
       "91                    -1.397419                          8.948276   \n",
       "88                    -0.907811                          9.449393   \n",
       "..                          ...                               ...   \n",
       "710                   -1.433349                          7.662791   \n",
       "315                   -4.490451                          7.623077   \n",
       "48                    -1.739112                          8.725000   \n",
       "274                   -1.192451                          7.904110   \n",
       "30                    -1.243985                         10.104972   \n",
       "\n",
       "     train_avg_losing_trade_duration  train_profit_factor  ...  \\\n",
       "751                         2.731707             8.745014  ...   \n",
       "598                         3.171875             4.160977  ...   \n",
       "162                         3.448718             4.014227  ...   \n",
       "91                          2.478261             2.793046  ...   \n",
       "88                          3.347826             5.855560  ...   \n",
       "..                               ...                  ...  ...   \n",
       "710                         3.068182             3.657179  ...   \n",
       "315                         2.428571             2.191529  ...   \n",
       "48                          2.550000             4.033304  ...   \n",
       "274                         3.142857             4.821732  ...   \n",
       "30                          3.602273             5.267913  ...   \n",
       "\n",
       "     train_true_precision  train_true_recall  train_true_f1_score  \\\n",
       "751                  0.96               0.97                 0.97   \n",
       "598                  0.92               0.93                 0.92   \n",
       "162                  0.90               0.95                 0.93   \n",
       "91                   0.95               0.98                 0.97   \n",
       "88                   0.92               0.92                 0.92   \n",
       "..                    ...                ...                  ...   \n",
       "710                  0.95               0.98                 0.96   \n",
       "315                  0.95               0.96                 0.96   \n",
       "48                   0.98               0.98                 0.98   \n",
       "274                  0.88               0.96                 0.92   \n",
       "30                   0.93               0.94                 0.94   \n",
       "\n",
       "     train_accuracy  train_macro avg_precision  train_macro avg_recall  \\\n",
       "751            0.97                       0.97                    0.97   \n",
       "598            0.94                       0.93                    0.94   \n",
       "162            0.95                       0.94                    0.95   \n",
       "91             0.97                       0.97                    0.97   \n",
       "88             0.93                       0.93                    0.93   \n",
       "..              ...                        ...                     ...   \n",
       "710            0.97                       0.97                    0.97   \n",
       "315            0.96                       0.96                    0.96   \n",
       "48             0.98                       0.98                    0.98   \n",
       "274            0.94                       0.93                    0.95   \n",
       "30             0.94                       0.94                    0.94   \n",
       "\n",
       "     train_macro avg_f1_score  train_weighted avg_precision  \\\n",
       "751                      0.97                          0.97   \n",
       "598                      0.94                          0.94   \n",
       "162                      0.94                          0.95   \n",
       "91                       0.97                          0.97   \n",
       "88                       0.93                          0.93   \n",
       "..                        ...                           ...   \n",
       "710                      0.97                          0.97   \n",
       "315                      0.96                          0.96   \n",
       "48                       0.98                          0.98   \n",
       "274                      0.94                          0.95   \n",
       "30                       0.94                          0.94   \n",
       "\n",
       "     train_weighted avg_recall  train_weighted avg_f1_score  \n",
       "751                       0.97                         0.97  \n",
       "598                       0.94                         0.94  \n",
       "162                       0.95                         0.95  \n",
       "91                        0.97                         0.97  \n",
       "88                        0.93                         0.93  \n",
       "..                         ...                          ...  \n",
       "710                       0.97                         0.97  \n",
       "315                       0.96                         0.96  \n",
       "48                        0.98                         0.98  \n",
       "274                       0.94                         0.95  \n",
       "30                        0.94                         0.94  \n",
       "\n",
       "[211 rows x 79 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_max_drawdown_[%]</th>\n",
       "      <th>train_max_drawdown_duration</th>\n",
       "      <th>train_win_rate_[%]</th>\n",
       "      <th>train_best_trade_[%]</th>\n",
       "      <th>train_worst_trade_[%]</th>\n",
       "      <th>train_avg_winning_trade_[%]</th>\n",
       "      <th>train_avg_losing_trade_[%]</th>\n",
       "      <th>train_avg_winning_trade_duration</th>\n",
       "      <th>train_avg_losing_trade_duration</th>\n",
       "      <th>train_profit_factor</th>\n",
       "      <th>...</th>\n",
       "      <th>train_true_precision</th>\n",
       "      <th>train_true_recall</th>\n",
       "      <th>train_true_f1_score</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_macro avg_precision</th>\n",
       "      <th>train_macro avg_recall</th>\n",
       "      <th>train_macro avg_f1_score</th>\n",
       "      <th>train_weighted avg_precision</th>\n",
       "      <th>train_weighted avg_recall</th>\n",
       "      <th>train_weighted avg_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.842808</td>\n",
       "      <td>0.323245</td>\n",
       "      <td>55.319149</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>-8.695652</td>\n",
       "      <td>4.503891</td>\n",
       "      <td>-1.626228</td>\n",
       "      <td>7.076923</td>\n",
       "      <td>2.803279</td>\n",
       "      <td>2.872888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.095343</td>\n",
       "      <td>0.212505</td>\n",
       "      <td>57.711443</td>\n",
       "      <td>34.033639</td>\n",
       "      <td>-7.347840</td>\n",
       "      <td>4.492853</td>\n",
       "      <td>-1.728899</td>\n",
       "      <td>7.146552</td>\n",
       "      <td>2.987952</td>\n",
       "      <td>2.461184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.871764</td>\n",
       "      <td>0.165587</td>\n",
       "      <td>69.709544</td>\n",
       "      <td>34.033639</td>\n",
       "      <td>-8.695652</td>\n",
       "      <td>3.846482</td>\n",
       "      <td>-1.791416</td>\n",
       "      <td>7.613095</td>\n",
       "      <td>3.408451</td>\n",
       "      <td>5.409532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.956289</td>\n",
       "      <td>0.133086</td>\n",
       "      <td>68.896321</td>\n",
       "      <td>47.899160</td>\n",
       "      <td>-8.808931</td>\n",
       "      <td>3.829873</td>\n",
       "      <td>-1.669257</td>\n",
       "      <td>7.349515</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>4.509006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.921668</td>\n",
       "      <td>0.111805</td>\n",
       "      <td>67.673716</td>\n",
       "      <td>47.899160</td>\n",
       "      <td>-9.358977</td>\n",
       "      <td>3.718765</td>\n",
       "      <td>-1.572589</td>\n",
       "      <td>8.245536</td>\n",
       "      <td>3.431373</td>\n",
       "      <td>5.106710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>61.077448</td>\n",
       "      <td>0.150085</td>\n",
       "      <td>63.448276</td>\n",
       "      <td>11.879766</td>\n",
       "      <td>-1.636512</td>\n",
       "      <td>0.931519</td>\n",
       "      <td>-0.360271</td>\n",
       "      <td>8.380435</td>\n",
       "      <td>2.849057</td>\n",
       "      <td>4.357045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>61.077448</td>\n",
       "      <td>0.106624</td>\n",
       "      <td>65.680473</td>\n",
       "      <td>11.991146</td>\n",
       "      <td>-1.650537</td>\n",
       "      <td>1.084922</td>\n",
       "      <td>-0.389273</td>\n",
       "      <td>10.945946</td>\n",
       "      <td>3.258621</td>\n",
       "      <td>5.857682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>61.077448</td>\n",
       "      <td>0.081131</td>\n",
       "      <td>71.304348</td>\n",
       "      <td>11.991146</td>\n",
       "      <td>-1.499235</td>\n",
       "      <td>0.868646</td>\n",
       "      <td>-0.396806</td>\n",
       "      <td>9.914634</td>\n",
       "      <td>3.227273</td>\n",
       "      <td>5.256213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>61.077448</td>\n",
       "      <td>0.073013</td>\n",
       "      <td>69.930070</td>\n",
       "      <td>12.515372</td>\n",
       "      <td>-1.283778</td>\n",
       "      <td>0.845167</td>\n",
       "      <td>-0.325988</td>\n",
       "      <td>9.340000</td>\n",
       "      <td>3.430233</td>\n",
       "      <td>5.686380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>61.077448</td>\n",
       "      <td>0.063147</td>\n",
       "      <td>64.906832</td>\n",
       "      <td>12.299707</td>\n",
       "      <td>-1.002768</td>\n",
       "      <td>0.953823</td>\n",
       "      <td>-0.307057</td>\n",
       "      <td>10.569378</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>6.080927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train_max_drawdown_[%]  train_max_drawdown_duration  train_win_rate_[%]  \\\n",
       "0                  32.842808                     0.323245           55.319149   \n",
       "1                  32.095343                     0.212505           57.711443   \n",
       "2                  31.871764                     0.165587           69.709544   \n",
       "3                  31.956289                     0.133086           68.896321   \n",
       "4                  32.921668                     0.111805           67.673716   \n",
       "...                      ...                          ...                 ...   \n",
       "1047               61.077448                     0.150085           63.448276   \n",
       "1048               61.077448                     0.106624           65.680473   \n",
       "1049               61.077448                     0.081131           71.304348   \n",
       "1050               61.077448                     0.073013           69.930070   \n",
       "1051               61.077448                     0.063147           64.906832   \n",
       "\n",
       "      train_best_trade_[%]  train_worst_trade_[%]  \\\n",
       "0                29.411765              -8.695652   \n",
       "1                34.033639              -7.347840   \n",
       "2                34.033639              -8.695652   \n",
       "3                47.899160              -8.808931   \n",
       "4                47.899160              -9.358977   \n",
       "...                    ...                    ...   \n",
       "1047             11.879766              -1.636512   \n",
       "1048             11.991146              -1.650537   \n",
       "1049             11.991146              -1.499235   \n",
       "1050             12.515372              -1.283778   \n",
       "1051             12.299707              -1.002768   \n",
       "\n",
       "      train_avg_winning_trade_[%]  train_avg_losing_trade_[%]  \\\n",
       "0                        4.503891                   -1.626228   \n",
       "1                        4.492853                   -1.728899   \n",
       "2                        3.846482                   -1.791416   \n",
       "3                        3.829873                   -1.669257   \n",
       "4                        3.718765                   -1.572589   \n",
       "...                           ...                         ...   \n",
       "1047                     0.931519                   -0.360271   \n",
       "1048                     1.084922                   -0.389273   \n",
       "1049                     0.868646                   -0.396806   \n",
       "1050                     0.845167                   -0.325988   \n",
       "1051                     0.953823                   -0.307057   \n",
       "\n",
       "      train_avg_winning_trade_duration  train_avg_losing_trade_duration  \\\n",
       "0                             7.076923                         2.803279   \n",
       "1                             7.146552                         2.987952   \n",
       "2                             7.613095                         3.408451   \n",
       "3                             7.349515                         3.200000   \n",
       "4                             8.245536                         3.431373   \n",
       "...                                ...                              ...   \n",
       "1047                          8.380435                         2.849057   \n",
       "1048                         10.945946                         3.258621   \n",
       "1049                          9.914634                         3.227273   \n",
       "1050                          9.340000                         3.430233   \n",
       "1051                         10.569378                         3.285714   \n",
       "\n",
       "      train_profit_factor  ...  train_true_precision  train_true_recall  \\\n",
       "0                2.872888  ...                  0.94               0.98   \n",
       "1                2.461184  ...                  0.92               0.97   \n",
       "2                5.409532  ...                  0.92               0.96   \n",
       "3                4.509006  ...                  0.91               0.94   \n",
       "4                5.106710  ...                  0.90               0.93   \n",
       "...                   ...  ...                   ...                ...   \n",
       "1047             4.357045  ...                  0.95               0.96   \n",
       "1048             5.857682  ...                  0.94               0.94   \n",
       "1049             5.256213  ...                  0.93               0.94   \n",
       "1050             5.686380  ...                  0.91               0.93   \n",
       "1051             6.080927  ...                  0.91               0.93   \n",
       "\n",
       "      train_true_f1_score  train_accuracy  train_macro avg_precision  \\\n",
       "0                    0.96            0.97                       0.96   \n",
       "1                    0.94            0.96                       0.95   \n",
       "2                    0.94            0.95                       0.95   \n",
       "3                    0.93            0.94                       0.94   \n",
       "4                    0.92            0.93                       0.93   \n",
       "...                   ...             ...                        ...   \n",
       "1047                 0.95            0.96                       0.96   \n",
       "1048                 0.94            0.95                       0.94   \n",
       "1049                 0.93            0.94                       0.94   \n",
       "1050                 0.92            0.93                       0.93   \n",
       "1051                 0.92            0.93                       0.93   \n",
       "\n",
       "      train_macro avg_recall  train_macro avg_f1_score  \\\n",
       "0                       0.97                      0.97   \n",
       "1                       0.96                      0.95   \n",
       "2                       0.95                      0.95   \n",
       "3                       0.94                      0.94   \n",
       "4                       0.93                      0.93   \n",
       "...                      ...                       ...   \n",
       "1047                    0.96                      0.96   \n",
       "1048                    0.95                      0.94   \n",
       "1049                    0.94                      0.94   \n",
       "1050                    0.93                      0.93   \n",
       "1051                    0.93                      0.93   \n",
       "\n",
       "      train_weighted avg_precision  train_weighted avg_recall  \\\n",
       "0                             0.97                       0.97   \n",
       "1                             0.96                       0.96   \n",
       "2                             0.95                       0.95   \n",
       "3                             0.94                       0.94   \n",
       "4                             0.93                       0.93   \n",
       "...                            ...                        ...   \n",
       "1047                          0.96                       0.96   \n",
       "1048                          0.95                       0.95   \n",
       "1049                          0.94                       0.94   \n",
       "1050                          0.93                       0.93   \n",
       "1051                          0.93                       0.93   \n",
       "\n",
       "      train_weighted avg_f1_score  \n",
       "0                            0.97  \n",
       "1                            0.96  \n",
       "2                            0.95  \n",
       "3                            0.94  \n",
       "4                            0.93  \n",
       "...                           ...  \n",
       "1047                         0.96  \n",
       "1048                         0.95  \n",
       "1049                         0.94  \n",
       "1050                         0.93  \n",
       "1051                         0.93  \n",
       "\n",
       "[1051 rows x 80 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751     SLB\n",
       "598    CSCO\n",
       "162     XLF\n",
       "91      MMM\n",
       "88      VTI\n",
       "       ... \n",
       "710      GL\n",
       "315    SOXL\n",
       "48      LUV\n",
       "274     MDT\n",
       "30      CVX\n",
       "Name: symbol, Length: 211, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[y_test.index,'symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>test_sharpe_ratio</th>\n",
       "      <th>test_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>SLB</td>\n",
       "      <td>1.651323</td>\n",
       "      <td>1.472009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>1.174150</td>\n",
       "      <td>1.444995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>XLF</td>\n",
       "      <td>1.014791</td>\n",
       "      <td>1.041099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>MMM</td>\n",
       "      <td>1.908073</td>\n",
       "      <td>1.819430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>VTI</td>\n",
       "      <td>0.727197</td>\n",
       "      <td>1.062746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>GL</td>\n",
       "      <td>0.906998</td>\n",
       "      <td>1.012290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>SOXL</td>\n",
       "      <td>3.025191</td>\n",
       "      <td>1.416757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LUV</td>\n",
       "      <td>0.709048</td>\n",
       "      <td>1.395481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>MDT</td>\n",
       "      <td>1.314761</td>\n",
       "      <td>1.097660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CVX</td>\n",
       "      <td>1.105388</td>\n",
       "      <td>1.114087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol  test_sharpe_ratio  test_pred\n",
       "751    SLB           1.651323   1.472009\n",
       "598   CSCO           1.174150   1.444995\n",
       "162    XLF           1.014791   1.041099\n",
       "91     MMM           1.908073   1.819430\n",
       "88     VTI           0.727197   1.062746\n",
       "..     ...                ...        ...\n",
       "710     GL           0.906998   1.012290\n",
       "315   SOXL           3.025191   1.416757\n",
       "48     LUV           0.709048   1.395481\n",
       "274    MDT           1.314761   1.097660\n",
       "30     CVX           1.105388   1.114087\n",
       "\n",
       "[211 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pf_symbols = pd.concat([df.loc[y_test.index,'symbol'], y_test, pd.Series(y_pred, index = y_test.index, name = 'test_pred')],\n",
    "                          axis = 1)\n",
    "df_pf_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pf_symbols['symbol'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PBR</th>\n",
       "      <td>1</td>\n",
       "      <td>3.127126</td>\n",
       "      <td>3.127126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>2</td>\n",
       "      <td>2.854154</td>\n",
       "      <td>2.854154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALE</th>\n",
       "      <td>1</td>\n",
       "      <td>2.476554</td>\n",
       "      <td>2.476554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTNT</th>\n",
       "      <td>1</td>\n",
       "      <td>2.351755</td>\n",
       "      <td>2.351755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BX</th>\n",
       "      <td>1</td>\n",
       "      <td>2.286201</td>\n",
       "      <td>2.286201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>4</td>\n",
       "      <td>2.212806</td>\n",
       "      <td>2.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>1</td>\n",
       "      <td>2.209271</td>\n",
       "      <td>2.209271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNA</th>\n",
       "      <td>1</td>\n",
       "      <td>2.206121</td>\n",
       "      <td>2.206121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSX</th>\n",
       "      <td>1</td>\n",
       "      <td>2.171333</td>\n",
       "      <td>2.171333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PANW</th>\n",
       "      <td>1</td>\n",
       "      <td>2.092743</td>\n",
       "      <td>2.092743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VZ</th>\n",
       "      <td>2</td>\n",
       "      <td>2.054548</td>\n",
       "      <td>2.054548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANET</th>\n",
       "      <td>1</td>\n",
       "      <td>2.045580</td>\n",
       "      <td>2.045580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBD</th>\n",
       "      <td>4</td>\n",
       "      <td>2.037855</td>\n",
       "      <td>2.064271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UPRO</th>\n",
       "      <td>1</td>\n",
       "      <td>2.037326</td>\n",
       "      <td>2.037326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IYR</th>\n",
       "      <td>2</td>\n",
       "      <td>2.024388</td>\n",
       "      <td>2.024388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>3</td>\n",
       "      <td>2.009567</td>\n",
       "      <td>2.094069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRK</th>\n",
       "      <td>2</td>\n",
       "      <td>1.990337</td>\n",
       "      <td>1.990337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOP</th>\n",
       "      <td>1</td>\n",
       "      <td>1.984830</td>\n",
       "      <td>1.984830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KEY</th>\n",
       "      <td>1</td>\n",
       "      <td>1.952256</td>\n",
       "      <td>1.952256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>1</td>\n",
       "      <td>1.878809</td>\n",
       "      <td>1.878809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSM</th>\n",
       "      <td>1</td>\n",
       "      <td>1.839341</td>\n",
       "      <td>1.839341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAG</th>\n",
       "      <td>1</td>\n",
       "      <td>1.825567</td>\n",
       "      <td>1.825567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMM</th>\n",
       "      <td>1</td>\n",
       "      <td>1.819430</td>\n",
       "      <td>1.819430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDLZ</th>\n",
       "      <td>1</td>\n",
       "      <td>1.795455</td>\n",
       "      <td>1.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HBAN</th>\n",
       "      <td>3</td>\n",
       "      <td>1.785659</td>\n",
       "      <td>1.631336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean    median\n",
       "symbol                           \n",
       "PBR         1  3.127126  3.127126\n",
       "T           2  2.854154  2.854154\n",
       "VALE        1  2.476554  2.476554\n",
       "FTNT        1  2.351755  2.351755\n",
       "BX          1  2.286201  2.286201\n",
       "MO          4  2.212806  2.265000\n",
       "NVDA        1  2.209271  2.209271\n",
       "TNA         1  2.206121  2.206121\n",
       "BSX         1  2.171333  2.171333\n",
       "PANW        1  2.092743  2.092743\n",
       "VZ          2  2.054548  2.054548\n",
       "ANET        1  2.045580  2.045580\n",
       "WBD         4  2.037855  2.064271\n",
       "UPRO        1  2.037326  2.037326\n",
       "IYR         2  2.024388  2.024388\n",
       "AMZN        3  2.009567  2.094069\n",
       "MRK         2  1.990337  1.990337\n",
       "SHOP        1  1.984830  1.984830\n",
       "KEY         1  1.952256  1.952256\n",
       "IBM         1  1.878809  1.878809\n",
       "TSM         1  1.839341  1.839341\n",
       "CAG         1  1.825567  1.825567\n",
       "MMM         1  1.819430  1.819430\n",
       "MDLZ        1  1.795455  1.795455\n",
       "HBAN        3  1.785659  1.631336"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pf_symbols.groupby(by = 'symbol')['test_pred']\\\n",
    "    .agg(['count', 'mean', 'median'])\\\n",
    "    .sort_values(by = 'mean', ascending = False)\\\n",
    "    .head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_calpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
